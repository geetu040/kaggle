{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sacrum/ml-labs-03-logistic-regression-from-scratch?scriptVersionId=178243455\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"10043cf1","metadata":{"papermill":{"duration":0.009359,"end_time":"2024-05-17T16:28:47.859569","exception":false,"start_time":"2024-05-17T16:28:47.85021","status":"completed"},"tags":[]},"source":["# 1. Logistic Regression\n","\n","In this section we will implement basic functions that are used for training a logistic regression model\n","\n","\n","\n","## Deriving the Logistic Loss Function\n","\n","In logistic regression, we aim to model the probability of a binary outcome (y) given a set of features (x). The logistic function (g(z)) transforms the linear combination of weights (w) and features (x) into a probability between 0 and 1. \n","\n","Here's how we derive the logistic loss function:\n","\n","**1. Noting the desired outcome:**\n","\n","The desired outcome is to have the predicted probability (Å·) closely match the actual label (y). If y is 1, we want Å· to be close to 1. Conversely, if y is 0, we want Å· to be close to 0.\n","\n","**2. Choosing a loss function:**\n","\n","A common choice for measuring the difference between the predicted and actual values in logistic regression is the **log loss** (also called binary cross-entropy). It penalizes the model for both underestimating and overestimating the true probability.\n","\n","**3. Formulating the loss function:**\n","\n","The log loss for a single data point is:\n","\n","```\n","L(y, Å·) = - (y * log(Å·) + (1 - y) * log(1 - Å·))\n","```\n","\n","where:\n","\n","* **L(y, Å·):** Loss for a single data point\n","* **y:** True label (0 or 1)\n","* **Å·:** Predicted probability\n","\n","**4. Explanation:**\n","\n","* When y = 1 and Å· is close to 1, the first term (-y * log(Å·)) dominates, resulting in a small loss.\n","* When y = 0 and Å· is close to 0, the second term (-(1 - y) * log(1 - Å·)) dominates, again resulting in a small loss.\n","* Conversely, if the model underestimates or overestimates the probability, the corresponding term in the loss function becomes larger, penalizing the model.\n","\n","**5. Average loss for multiple data points:**\n","\n","To evaluate the model's performance on the entire dataset, we calculate the average loss over all data points:\n","\n","```\n","Loss = - (1/N) * sum(L(y_i, Å·_i)) for i in N data points\n","```\n","\n","This average loss is used during the training process to optimize the model's weights (w) using gradient descent algorithms. By minimizing the average loss, the model learns to predict probabilities that better match the true labels.\n","\n","## Deriving the Update Rule for Logistic Regression\n","\n","The update rule for logistic regression uses the **gradient descent algorithm** to adjust the model's weights (w) based on the calculated loss function. Here's the derivation:\n","\n","**1. Gradient of the loss function:**\n","\n","The update rule involves taking the negative gradient of the loss function with respect to each weight (w_j). This indicates the direction in which we should adjust the weights to minimize the loss.\n","\n","For the logistic loss function, the gradient for a single data point is:\n","\n","```\n","âˆ‡_w L(y, Å·) = (Å· - y) * Ï†_j(x)\n","```\n","\n","where:\n","\n","* **âˆ‡_w:** Gradient with respect to weight vector w\n","* **Ï†_j(x):** j-th feature in the feature vector Ï•(x)\n","\n","**2. Update rule using gradient descent:**\n","\n","Following the principle of gradient descent, we update each weight by subtracting the learning rate (Î·) multiplied by the gradient:\n","\n","```\n","w_j_new = w_j_old - Î· * âˆ‡_w L(y, Å·)\n","```\n","\n","Substituting the gradient expression:\n","\n","```\n","w_j_new = w_j_old - Î· * (Å· - y) * Ï†_j(x)\n","```\n","\n","**3. Update for all data points:**\n","\n","To update the weights based on the entire dataset, we perform the update rule for each data point and average the updates:\n","\n","```\n","w_j_new = w_j_old - Î· * (1/N) * sum((Å·_i - y_i) * Ï†_j(x_i)) for i in N data points\n","```\n","\n","This update rule iteratively adjusts the weights based on the difference between the predicted and actual values, leading the model towards minimizing the overall loss and improving its performance in predicting probabilities for future data.\n","\n","## Dataset\n","\n","| Feature | Individual 1 | Individual 2 | Individual 3 | Individual 4 | Individual 5 | Individual 6 |\n","|---|---|---|---|---|---|---|\n","| Income (in Lacs) | 4.5 | 8.5 | 6 | 3 | 10 | 5.5 |\n","| Age | 34 | 42 | 29 | 24 | 50 | 36 |  \n","| Debt (in Lacs) | 10 | 15 | 20 | 5 | 30 |8 |\n","| Experience (Years) | 5 | 10 | 4 | 2 | 20 | 6 |\n","| Approval | No | Yes | No | No | Yes | No |\n","\n","## Gradient Descent\n","\n","These are the steps to gradient descent update rule for logistic regression:\n","\n","1. **Initialize weights**: Start with some initial values for the weights.\n","\n","2. **Set learning rate**: Choose a small number (e.g., 0.01) as the learning rate. This determines how much the weights are updated in each iteration.\n","\n","3. **Set number of iterations**: Decide how many times you want to update the weights (e.g., 1000 times).\n","\n","4. **Set batch size**: Define the number of data points you want to use in each batch update (e.g., 32).\n","\n","5. **For each epoch** (complete pass through the data):\n","   - For each batch of data (of size batch_size):\n","     - Compute the gradient of the loss function with respect to each weight.\n","     - Update each weight by subtracting the product of the learning rate, the gradient, and the inverse of the batch size.\n","\n","6. Repeat the above step for the specified number of epochs.\n","\n","## Applying Gradient Descent\n","\n","W = [0, 0, 0, 0, 0, 0]\n","\n","Å· = ð‘”(ð’˜ð‘‡ ðœ‘(ð’™))\n","\n","L(y, Å·) = - (y * log(Å·) + (1 - y) * log(1 - Å·))\n","\n","Loss = - (1/N) * sum(L(y_i, Å·_i)) for i in N data points\n","\n","âˆ‡_w L(y, Å·) = (Å· - y) * Ï†_j(x)\n","\n","w_j_new = w_j_old - Î· * âˆ‡_w L(y, Å·)\n","\n","### Apply Batch GD\n","\n","> When batch size is equal to the number of samples in data, gradient descent become batch gradient descent\n","\n","Epochs = 5\n","\t  \n","Batch Size = 6\n","\n","Learning Rate = 0.001\n","\n","- Epoch 1\n","    - Iteration 1\n","        - Å· = [0.5 0.5 0.5 0.5 0.5 0.5]\n","        - loss = 0.693\n","        - gradient = [ 0.042  2.583 -0.167 -1.083]\n","        - w = [-0.    -0.003  0.     0.001]\n","- Epoch 2\n","    - Iteration 1\n","        - Å· = [0.48  0.476 0.483 0.485 0.474 0.479]\n","        - loss = 0.686\n","        - gradient = [-0.094  1.819 -0.486 -1.263]\n","        - w = [ 0.    -0.004  0.001  0.002]\n","- Epoch 3\n","    - Iteration 1\n","        - Å· = [0.467 0.462 0.474 0.476 0.462 0.465]\n","        - loss = 0.681\n","        - gradient = [-0.17   1.383 -0.66  -1.361]\n","        - w = [ 0.    -0.006  0.001  0.004]\n","- Epoch 4\n","    - Iteration 1\n","        - Å· = [0.459 0.454 0.469 0.469 0.457 0.457]\n","        - loss = 0.677\n","        - gradient = [-0.213  1.133 -0.754 -1.413]\n","        - w = [ 0.    -0.007  0.002  0.005]\n","- Epoch 5\n","    - Iteration 1\n","        - Å· = [0.453 0.449 0.466 0.464 0.456 0.45 ]\n","        - loss = 0.673\n","        - gradient = [-0.237  0.988 -0.802 -1.439]\n","        - w = [ 0.001 -0.008  0.003  0.007]\n","\n","### Apply Stochastic GD\n","\n","> When batch size is equal to the 1, gradient descent become Stochastic gradient descent\n","\n","Epochs = 2\n","\t  \n","Batch Size = 1\n","\n","Learning Rate = 0.001\n","\n","- Epoch 1\n","    - Iteration 1\n","        - Å· = [0.5]\n","        - loss = 0.693\n","        - gradient = [ 2.25 17.    5.    2.5 ]\n","        - w = [-0.002 -0.017 -0.005 -0.002]\n","    - Iteration 2\n","        - Å· = [0.303]\n","        - loss = 1.194\n","        - gradient = [ -5.925 -29.275 -10.455  -6.97 ]\n","        - w = [0.004 0.012 0.005 0.004]\n","    - Iteration 3\n","        - Å· = [0.624]\n","        - loss = 0.977\n","        - gradient = [ 3.742 18.085 12.473  2.495]\n","        - w = [-0.    -0.006 -0.007  0.002]\n","    - Iteration 4\n","        - Å· = [0.457]\n","        - loss = 0.611\n","        - gradient = [ 1.372 10.978  2.287  0.915]\n","        - w = [-0.001 -0.017 -0.009  0.001]\n","    - Iteration 5\n","        - Å· = [0.248]\n","        - loss = 1.396\n","        - gradient = [ -7.525 -37.623 -22.574 -15.049]\n","        - w = [0.006 0.021 0.013 0.016]\n","    - Iteration 6\n","        - Å· = [0.728]\n","        - loss = 1.303\n","        - gradient = [ 4.006 26.221  5.827  4.37 ]\n","        - w = [ 0.002 -0.005  0.007  0.012]\n","- Epoch 2\n","    - Iteration 1\n","        - Å· = [0.49]\n","        - loss = 0.673\n","        - gradient = [ 2.204 16.654  4.898  2.449]\n","        - w = [-0.    -0.022  0.003  0.009]\n","    - Iteration 2\n","        - Å· = [0.311]\n","        - loss = 1.168\n","        - gradient = [ -5.857 -28.94  -10.336  -6.891]\n","        - w = [0.006 0.007 0.013 0.016]\n","    - Iteration 3\n","        - Å· = [0.636]\n","        - loss = 1.01\n","        - gradient = [ 3.814 18.436 12.714  2.543]\n","        - w = [ 0.002 -0.012  0.     0.014]\n","    - Iteration 4\n","        - Å· = [0.44]\n","        - loss = 0.579\n","        - gradient = [ 1.319 10.549  2.198  0.879]\n","        - w = [ 0.001 -0.022 -0.002  0.013]\n","    - Iteration 5\n","        - Å· = [0.288]\n","        - loss = 1.244\n","        - gradient = [ -7.118 -35.589 -21.353 -14.236]\n","        - w = [0.008 0.014 0.019 0.027]\n","    - Iteration 6\n","        - Å· = [0.7]\n","        - loss = 1.202\n","        - gradient = [ 3.847 25.184  5.596  4.197]\n","        - w = [ 0.004 -0.012  0.014  0.023]\n","\n","## Evaluation\n","\n","Accuracy = Number of correct predictions / Total number of predictions\n","\n","For Batch GD\n","```\n","preds = [0.449 0.446 0.465 0.46  0.457 0.446]\n","y = [0 1 0 0 1 0]\n","accuracy = 0.67\n","```\n","\n","For Stochastic GD\n","```\n","preds = [0.468 0.494 0.513 0.461 0.58  0.462]\n","y = [0 1 0 0 1 0]\n","accuracy = 0.67\n","```\n","\n","| Gradient Descent Method | Accuracy | Epochs | Batch Size | Learning Rate |\n","|--------------------------|----------|--------|------------|---------------|\n","| Batch GD                 | 0.67     | 5      | 6          | 0.001         |\n","| Stochastic GD            | 0.67     | 2      | 1          | 0.001         |\n","\n","\n","Both Batch Gradient Descent (Batch GD) and Stochastic Gradient Descent (Stochastic GD) achieved the same accuracy of 0.67. However, they differ in their training approach:\n","\n","1. **Batch GD**:\n","   - Uses a batch size of 6, meaning it processes 6 examples at a time before updating the weights.\n","   - Runs for 5 epochs, meaning it goes through the entire dataset 5 times.\n","   - Generally more stable but may take longer to converge due to processing the entire dataset before updating weights.\n","\n","2. **Stochastic GD**:\n","   - Uses a batch size of 1, meaning it updates the weights after processing each example.\n","   - Runs for 2 epochs, meaning it goes through the entire dataset twice.\n","   - Can be faster to update weights but may be more noisy and less stable due to the frequent updates.\n","\n","In this specific case, both methods achieved the same accuracy, but Batch GD took longer to train due to processing larger batches and more epochs. Stochastic GD trained faster but may be more sensitive to noise in the data."]},{"cell_type":"markdown","id":"01fe783b","metadata":{"papermill":{"duration":0.008793,"end_time":"2024-05-17T16:28:47.877288","exception":false,"start_time":"2024-05-17T16:28:47.868495","status":"completed"},"tags":[]},"source":["## 2. Implementation in Code"]},{"cell_type":"markdown","id":"fbedaf5c","metadata":{"papermill":{"duration":0.008613,"end_time":"2024-05-17T16:28:47.894771","exception":false,"start_time":"2024-05-17T16:28:47.886158","status":"completed"},"tags":[]},"source":["### Setting Up"]},{"cell_type":"code","execution_count":1,"id":"6e48e2c9","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:47.915677Z","iopub.status.busy":"2024-05-17T16:28:47.914999Z","iopub.status.idle":"2024-05-17T16:28:47.925672Z","shell.execute_reply":"2024-05-17T16:28:47.924371Z"},"papermill":{"duration":0.024554,"end_time":"2024-05-17T16:28:47.928218","exception":false,"start_time":"2024-05-17T16:28:47.903664","status":"completed"},"tags":[]},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":2,"id":"0f62c42c","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:47.94814Z","iopub.status.busy":"2024-05-17T16:28:47.947744Z","iopub.status.idle":"2024-05-17T16:28:47.952489Z","shell.execute_reply":"2024-05-17T16:28:47.951739Z"},"papermill":{"duration":0.017006,"end_time":"2024-05-17T16:28:47.954436","exception":false,"start_time":"2024-05-17T16:28:47.93743","status":"completed"},"tags":[]},"outputs":[],"source":["# this function prints the vector with its shape\n","\n","def print_vector(vector, vector_name):\n","\tprint(f\"\"\">> {vector_name}.shape\\n{vector.shape}\\n\\n>> {vector_name}\\n{vector}\\n\"\"\")"]},{"cell_type":"code","execution_count":3,"id":"437ddc91","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:47.975524Z","iopub.status.busy":"2024-05-17T16:28:47.974532Z","iopub.status.idle":"2024-05-17T16:28:47.981247Z","shell.execute_reply":"2024-05-17T16:28:47.980529Z"},"papermill":{"duration":0.019584,"end_time":"2024-05-17T16:28:47.983278","exception":false,"start_time":"2024-05-17T16:28:47.963694","status":"completed"},"tags":[]},"outputs":[],"source":["# Initialize Dummy Data\n","\n","n_samples = 10\n","n_inp_dims = 4\n","n_out_dims = 2\n","\n","weight = np.random.rand(n_inp_dims, n_out_dims)\n","feature_vector = np.random.rand(n_samples, n_inp_dims)\n","y_true = np.random.randint(0, 2, size=(n_samples, n_out_dims))"]},{"cell_type":"code","execution_count":4,"id":"36ff3016","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:48.004391Z","iopub.status.busy":"2024-05-17T16:28:48.002789Z","iopub.status.idle":"2024-05-17T16:28:48.012226Z","shell.execute_reply":"2024-05-17T16:28:48.010671Z"},"papermill":{"duration":0.022021,"end_time":"2024-05-17T16:28:48.014567","exception":false,"start_time":"2024-05-17T16:28:47.992546","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":[">> weight.shape\n","(4, 2)\n","\n",">> weight\n","[[0.08462148 0.08141992]\n"," [0.45315634 0.04563395]\n"," [0.15407874 0.61881182]\n"," [0.11324862 0.0707232 ]]\n","\n",">> feature_vector.shape\n","(10, 4)\n","\n",">> feature_vector\n","[[0.4881384  0.09957008 0.56177337 0.44587574]\n"," [0.3978054  0.14561364 0.12273675 0.77941388]\n"," [0.13857465 0.68819973 0.32443448 0.49721472]\n"," [0.63147704 0.47253981 0.71493472 0.09589861]\n"," [0.16562338 0.80759302 0.31699851 0.51826292]\n"," [0.92587542 0.87732898 0.84075179 0.63889094]\n"," [0.79657433 0.30944903 0.18860998 0.3099062 ]\n"," [0.8262513  0.12186659 0.58946221 0.18716427]\n"," [0.64662448 0.17446101 0.50208342 0.23448922]\n"," [0.84886928 0.88548916 0.52341297 0.69545429]]\n","\n",">> y_true.shape\n","(10, 2)\n","\n",">> y_true\n","[[0 0]\n"," [0 1]\n"," [0 1]\n"," [1 0]\n"," [0 1]\n"," [1 0]\n"," [0 0]\n"," [0 0]\n"," [0 1]\n"," [0 1]]\n","\n"]}],"source":["print_vector(weight, \"weight\")\n","print_vector(feature_vector, \"feature_vector\")\n","print_vector(y_true, \"y_true\")"]},{"cell_type":"markdown","id":"2662fb1f","metadata":{"papermill":{"duration":0.009104,"end_time":"2024-05-17T16:28:48.033345","exception":false,"start_time":"2024-05-17T16:28:48.024241","status":"completed"},"tags":[]},"source":["### Logistic regression single prediction\n","Create a function that takes two vectors as input (weight and feature vector) and returns the logistic regression prediction for that input."]},{"cell_type":"code","execution_count":5,"id":"9bc0496f","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:48.053663Z","iopub.status.busy":"2024-05-17T16:28:48.053253Z","iopub.status.idle":"2024-05-17T16:28:48.058562Z","shell.execute_reply":"2024-05-17T16:28:48.05737Z"},"papermill":{"duration":0.018046,"end_time":"2024-05-17T16:28:48.060813","exception":false,"start_time":"2024-05-17T16:28:48.042767","status":"completed"},"tags":[]},"outputs":[],"source":["def logistic_regression_prediction(weight, feature_vector):\n","    z = np.dot(feature_vector, weight)\n","    return 1 / (1 + np.exp(-z))"]},{"cell_type":"code","execution_count":6,"id":"13472c53","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:48.081375Z","iopub.status.busy":"2024-05-17T16:28:48.080994Z","iopub.status.idle":"2024-05-17T16:28:48.085899Z","shell.execute_reply":"2024-05-17T16:28:48.085Z"},"papermill":{"duration":0.017558,"end_time":"2024-05-17T16:28:48.087847","exception":false,"start_time":"2024-05-17T16:28:48.070289","status":"completed"},"tags":[]},"outputs":[],"source":["single_predictions = logistic_regression_prediction(weight, feature_vector[0])"]},{"cell_type":"code","execution_count":7,"id":"9609a5dc","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:48.108896Z","iopub.status.busy":"2024-05-17T16:28:48.108293Z","iopub.status.idle":"2024-05-17T16:28:48.113404Z","shell.execute_reply":"2024-05-17T16:28:48.112246Z"},"papermill":{"duration":0.018226,"end_time":"2024-05-17T16:28:48.115471","exception":false,"start_time":"2024-05-17T16:28:48.097245","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":[">> single_predictions.shape\n","(2,)\n","\n",">> single_predictions\n","[0.55563862 0.6043094 ]\n","\n"]}],"source":["print_vector(single_predictions, \"single_predictions\")"]},{"cell_type":"markdown","id":"197db1cb","metadata":{"papermill":{"duration":0.009046,"end_time":"2024-05-17T16:28:48.13419","exception":false,"start_time":"2024-05-17T16:28:48.125144","status":"completed"},"tags":[]},"source":["### Logistic regression vector prediction\n","Create a function that takes a matrix and a vector as input (weight vector and feature matrix) and returns the logistic regression prediction vector for the whole training set."]},{"cell_type":"code","execution_count":8,"id":"dbcdc2d3","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:48.155051Z","iopub.status.busy":"2024-05-17T16:28:48.154458Z","iopub.status.idle":"2024-05-17T16:28:48.159018Z","shell.execute_reply":"2024-05-17T16:28:48.157915Z"},"papermill":{"duration":0.017518,"end_time":"2024-05-17T16:28:48.161174","exception":false,"start_time":"2024-05-17T16:28:48.143656","status":"completed"},"tags":[]},"outputs":[],"source":["def logistic_regression_vector_prediction(weight_vector, feature_matrix):\n","    z = np.dot(feature_matrix, weight_vector)\n","    return 1 / (1 + np.exp(-z))"]},{"cell_type":"code","execution_count":9,"id":"9b863b29","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:48.181949Z","iopub.status.busy":"2024-05-17T16:28:48.181591Z","iopub.status.idle":"2024-05-17T16:28:48.187838Z","shell.execute_reply":"2024-05-17T16:28:48.18702Z"},"papermill":{"duration":0.018953,"end_time":"2024-05-17T16:28:48.189786","exception":false,"start_time":"2024-05-17T16:28:48.170833","status":"completed"},"tags":[]},"outputs":[],"source":["predictions = logistic_regression_vector_prediction(weight, feature_vector)"]},{"cell_type":"code","execution_count":10,"id":"bcb07d31","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:48.211097Z","iopub.status.busy":"2024-05-17T16:28:48.210717Z","iopub.status.idle":"2024-05-17T16:28:48.21626Z","shell.execute_reply":"2024-05-17T16:28:48.214877Z"},"papermill":{"duration":0.018375,"end_time":"2024-05-17T16:28:48.218381","exception":false,"start_time":"2024-05-17T16:28:48.200006","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":[">> predictions.shape\n","(10, 2)\n","\n",">> predictions\n","[[0.55563862 0.6043094 ]\n"," [0.55152329 0.5424247 ]\n"," [0.6058464  0.56920701]\n"," [0.59594264 0.62765527]\n"," [0.61952121 0.57031938]\n"," [0.6632501  0.66393335]\n"," [0.56753311 0.55418884]\n"," [0.55901438 0.61083836]\n"," [0.5591449  0.59577428]\n"," [0.65305694 0.61836214]]\n","\n"]}],"source":["print_vector(predictions, \"predictions\")"]},{"cell_type":"markdown","id":"d1591d26","metadata":{"papermill":{"duration":0.009291,"end_time":"2024-05-17T16:28:48.237361","exception":false,"start_time":"2024-05-17T16:28:48.22807","status":"completed"},"tags":[]},"source":["### Logistic Loss\n","Now create a function that takes a vector of predictions and a vector of actual values as input and returns the Logistic Loss."]},{"cell_type":"code","execution_count":11,"id":"1399e115","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:48.258176Z","iopub.status.busy":"2024-05-17T16:28:48.257742Z","iopub.status.idle":"2024-05-17T16:28:48.263095Z","shell.execute_reply":"2024-05-17T16:28:48.261841Z"},"papermill":{"duration":0.018144,"end_time":"2024-05-17T16:28:48.265118","exception":false,"start_time":"2024-05-17T16:28:48.246974","status":"completed"},"tags":[]},"outputs":[],"source":["def logistic_loss(predictions, actual_values):\n","    epsilon = 1e-15\n","    return np.mean(\n","        - (actual_values * np.log(predictions + epsilon)\n","        + (1 - actual_values) * np.log(1 - predictions + epsilon))\n","    )\n","    # return -np.sum(actual_values * np.log(predictions))"]},{"cell_type":"code","execution_count":12,"id":"e4584911","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:48.286094Z","iopub.status.busy":"2024-05-17T16:28:48.285706Z","iopub.status.idle":"2024-05-17T16:28:48.290066Z","shell.execute_reply":"2024-05-17T16:28:48.289105Z"},"papermill":{"duration":0.017127,"end_time":"2024-05-17T16:28:48.291936","exception":false,"start_time":"2024-05-17T16:28:48.274809","status":"completed"},"tags":[]},"outputs":[],"source":["loss = logistic_loss(predictions, y_true)"]},{"cell_type":"code","execution_count":13,"id":"63988b05","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:48.312919Z","iopub.status.busy":"2024-05-17T16:28:48.312546Z","iopub.status.idle":"2024-05-17T16:28:48.317388Z","shell.execute_reply":"2024-05-17T16:28:48.316371Z"},"papermill":{"duration":0.017609,"end_time":"2024-05-17T16:28:48.319288","exception":false,"start_time":"2024-05-17T16:28:48.301679","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":[">> loss.shape\n","()\n","\n",">> loss\n","0.7732834723198788\n","\n"]}],"source":["print_vector(loss, \"loss\")"]},{"cell_type":"markdown","id":"c90058cf","metadata":{"papermill":{"duration":0.009543,"end_time":"2024-05-17T16:28:48.338743","exception":false,"start_time":"2024-05-17T16:28:48.3292","status":"completed"},"tags":[]},"source":["### Logistic Gradient\n","Now create a function that takes a vector of predictions and a vector of actual values as input and returns the Gradient of Logistic Loss."]},{"cell_type":"code","execution_count":14,"id":"bd87de05","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:48.360345Z","iopub.status.busy":"2024-05-17T16:28:48.359958Z","iopub.status.idle":"2024-05-17T16:28:48.364721Z","shell.execute_reply":"2024-05-17T16:28:48.363708Z"},"papermill":{"duration":0.017944,"end_time":"2024-05-17T16:28:48.366602","exception":false,"start_time":"2024-05-17T16:28:48.348658","status":"completed"},"tags":[]},"outputs":[],"source":["def logistic_gradient(feature_matrix, predictions, actual_values):\n","    n = len(predictions)\n","    return np.dot(feature_matrix.T, predictions - actual_values) / n"]},{"cell_type":"code","execution_count":15,"id":"75f187b9","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:48.388565Z","iopub.status.busy":"2024-05-17T16:28:48.387897Z","iopub.status.idle":"2024-05-17T16:28:48.393053Z","shell.execute_reply":"2024-05-17T16:28:48.391848Z"},"papermill":{"duration":0.01836,"end_time":"2024-05-17T16:28:48.395067","exception":false,"start_time":"2024-05-17T16:28:48.376707","status":"completed"},"tags":[]},"outputs":[],"source":["grad = logistic_gradient(feature_vector, predictions, y_true)"]},{"cell_type":"code","execution_count":16,"id":"9d33af7c","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:48.416687Z","iopub.status.busy":"2024-05-17T16:28:48.416302Z","iopub.status.idle":"2024-05-17T16:28:48.421636Z","shell.execute_reply":"2024-05-17T16:28:48.420605Z"},"papermill":{"duration":0.018912,"end_time":"2024-05-17T16:28:48.424042","exception":false,"start_time":"2024-05-17T16:28:48.40513","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":[">> grad.shape\n","(4, 2)\n","\n",">> grad\n","[[ 0.19401336  0.13539837]\n"," [ 0.14860966  0.00666195]\n"," [ 0.12598983  0.10761695]\n"," [ 0.19118205 -0.01138301]]\n","\n"]}],"source":["print_vector(grad, \"grad\")"]},{"cell_type":"markdown","id":"82d0307a","metadata":{"papermill":{"duration":0.010018,"end_time":"2024-05-17T16:28:48.444773","exception":false,"start_time":"2024-05-17T16:28:48.434755","status":"completed"},"tags":[]},"source":["### Gradient Descent Algorithm\n","This function performs batch gradient descent to optimize a linear regression model."]},{"cell_type":"code","execution_count":17,"id":"afe9eb34","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:48.466944Z","iopub.status.busy":"2024-05-17T16:28:48.466589Z","iopub.status.idle":"2024-05-17T16:28:48.475539Z","shell.execute_reply":"2024-05-17T16:28:48.474469Z"},"papermill":{"duration":0.022615,"end_time":"2024-05-17T16:28:48.477711","exception":false,"start_time":"2024-05-17T16:28:48.455096","status":"completed"},"tags":[]},"outputs":[],"source":["\n","\"\"\"\n","Batch gradient descent: batch_size=len(X)\n","Stochastic gradient descent: batch_size=1\n","Mini-batch gradient descent: batch_size=32\n","\"\"\"\n","\n","def gradient_descent(\n","\t\tX_train, y_train,\n","\t\tX_test=None, y_test=None,\n","\t\tbatch_size=32, epochs=10, learning_rate=0.01):\n","\n","\tn, m = X_train.shape\n","\to = y_train.shape[-1]\n","\n","\t# initialize random weights\n","\tweights = np.random.rand(m, o)\n","\n","\tfor epoch in range(epochs):\n","\n","\t\t# TRAINING\n","\t\ttrain_loss = 0\n","\t\tfor iteration in range(0, n, batch_size):\n","\n","\t\t\tbatch_start = iteration\n","\t\t\tbatch_end = iteration + batch_size\n","\n","\t\t\tx_batch = X_train[batch_start:batch_end]\n","\t\t\ty_batch = y_train[batch_start:batch_end]\n","\n","\t\t\tpredictions = logistic_regression_vector_prediction(weights, x_batch)\n","\n","\t\t\tgradient = logistic_gradient(x_batch, predictions, y_batch)\n","\t\t\tweights -= learning_rate * gradient\n","\t\t\t\n","\t\t\tbatch_loss = logistic_loss(predictions, y_batch)\n","\t\t\ttrain_loss += batch_loss\n","\n","\t\t# TESTING\n","\t\ttest_loss = None\n","\t\tif X_test is not None and y_test is not None:\n","\t\t\tpredictions = logistic_regression_vector_prediction(weights, X_test)\n","\t\t\ttest_loss = logistic_loss(predictions, y_test)\n","\n","\t\tprint(f\"epoch {epoch+1}/{epochs} | Train Loss {train_loss} | Test Loss {test_loss}\")\n","\n","\treturn weights"]},{"cell_type":"code","execution_count":18,"id":"c3bb3c3f","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:48.499789Z","iopub.status.busy":"2024-05-17T16:28:48.499428Z","iopub.status.idle":"2024-05-17T16:28:48.505228Z","shell.execute_reply":"2024-05-17T16:28:48.5044Z"},"papermill":{"duration":0.019165,"end_time":"2024-05-17T16:28:48.507173","exception":false,"start_time":"2024-05-17T16:28:48.488008","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 1/10 | Train Loss 1.039921165423216 | Test Loss None\n","epoch 2/10 | Train Loss 0.9578626259001194 | Test Loss None\n","epoch 3/10 | Train Loss 0.8898419841054629 | Test Loss None\n","epoch 4/10 | Train Loss 0.834478803597446 | Test Loss None\n","epoch 5/10 | Train Loss 0.7900042757312599 | Test Loss None\n","epoch 6/10 | Train Loss 0.75454341208784 | Test Loss None\n","epoch 7/10 | Train Loss 0.7263325811142264 | Test Loss None\n","epoch 8/10 | Train Loss 0.7038382545361648 | Test Loss None\n","epoch 9/10 | Train Loss 0.6857936552926904 | Test Loss None\n","epoch 10/10 | Train Loss 0.6711863321124855 | Test Loss None\n"]}],"source":["weights = gradient_descent(feature_vector, y_true, learning_rate=0.5, batch_size=len(y_true))"]},{"cell_type":"markdown","id":"388f2154","metadata":{"papermill":{"duration":0.010002,"end_time":"2024-05-17T16:28:48.527534","exception":false,"start_time":"2024-05-17T16:28:48.517532","status":"completed"},"tags":[]},"source":["# 3. MNIST Dataset\n","In this section we will apply Logistic Regression functions implemented from scracth above, on MNIST Dataset to predict handwritten digits"]},{"cell_type":"code","execution_count":19,"id":"cce32167","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:28:48.549715Z","iopub.status.busy":"2024-05-17T16:28:48.549363Z","iopub.status.idle":"2024-05-17T16:30:24.361693Z","shell.execute_reply":"2024-05-17T16:30:24.360777Z"},"papermill":{"duration":95.836648,"end_time":"2024-05-17T16:30:24.374447","exception":false,"start_time":"2024-05-17T16:28:48.537799","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train.shape: (50000, 784)\n","y_train.shape: (50000, 10)\n","X_test.shape: (20000, 784)\n","y_test.shape: (20000, 10)\n"]}],"source":["from sklearn.datasets import fetch_openml\n","import pandas as pd\n","\n","# Load the MNIST dataset\n","mnist = fetch_openml('mnist_784', version=1, cache=True, parser='auto')\n","\n","# Pandas data frame with feature vectors\n","X = mnist.data\n","\n","# Scale pixel values\n","X = X / 255.\n","\n","# Labels\n","y = mnist.target\n","\n","# Labels converted to integers\n","y = y.astype(int)\n","\n","# one hot encoding\n","y = pd.get_dummies(y).astype(int)\n","\n","# train test split\n","X_train = X.iloc[:50_000]\n","X_test = X.iloc[50_000:]\n","y_train = y.iloc[:50_000]\n","y_test = y.iloc[50_000:]\n","\n","# show shapes\n","print(\"X_train.shape:\", X_train.shape)\n","print(\"y_train.shape:\", y_train.shape)\n","print(\"X_test.shape:\", X_test.shape)\n","print(\"y_test.shape:\", y_test.shape)"]},{"cell_type":"markdown","id":"0e82463d","metadata":{"papermill":{"duration":0.010292,"end_time":"2024-05-17T16:30:24.395201","exception":false,"start_time":"2024-05-17T16:30:24.384909","status":"completed"},"tags":[]},"source":["### Applying Gradient Descents"]},{"cell_type":"code","execution_count":20,"id":"8c74fb4e","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:30:24.418336Z","iopub.status.busy":"2024-05-17T16:30:24.41756Z","iopub.status.idle":"2024-05-17T16:30:31.50086Z","shell.execute_reply":"2024-05-17T16:30:31.499442Z"},"papermill":{"duration":7.098864,"end_time":"2024-05-17T16:30:31.504413","exception":false,"start_time":"2024-05-17T16:30:24.405549","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 1/10 | Train Loss 29.92379987013802 | Test Loss 21.43426526031631\n","epoch 2/10 | Train Loss 21.43426526031631 | Test Loss 2.7671093594338876\n","epoch 3/10 | Train Loss 2.7671093594338876 | Test Loss 1.1337505894001068\n","epoch 4/10 | Train Loss 1.1337505894001068 | Test Loss 0.9966317265374365\n","epoch 5/10 | Train Loss 0.9966317265374365 | Test Loss 0.877089545073934\n","epoch 6/10 | Train Loss 0.877089545073934 | Test Loss 0.7739913087844714\n","epoch 7/10 | Train Loss 0.7739913087844714 | Test Loss 0.6864732465206014\n","epoch 8/10 | Train Loss 0.6864732465206014 | Test Loss 0.612491545949527\n","epoch 9/10 | Train Loss 0.612491545949527 | Test Loss 0.5502463608151078\n","epoch 10/10 | Train Loss 0.5502463608151078 | Test Loss 0.49813874870251657\n"]}],"source":["batch_gd_weights = gradient_descent(\n","\tX_train, y_train,\n","\tX_train, y_train,\n","\tbatch_size=len(X_train),\n","\tepochs=10,\n","\tlearning_rate=0.8,\n",")"]},{"cell_type":"code","execution_count":21,"id":"5588b92b","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:30:31.550358Z","iopub.status.busy":"2024-05-17T16:30:31.549866Z","iopub.status.idle":"2024-05-17T16:30:39.6394Z","shell.execute_reply":"2024-05-17T16:30:39.638188Z"},"papermill":{"duration":8.111214,"end_time":"2024-05-17T16:30:39.642222","exception":false,"start_time":"2024-05-17T16:30:31.531008","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 1/10 | Train Loss 71.03182866618727 | Test Loss 0.16451235623137758\n","epoch 2/10 | Train Loss 7.138745492583239 | Test Loss 0.1301894949651112\n","epoch 3/10 | Train Loss 6.096256921413664 | Test Loss 0.11716068197117482\n","epoch 4/10 | Train Loss 5.609873843532675 | Test Loss 0.10979591875931535\n","epoch 5/10 | Train Loss 5.310776406442182 | Test Loss 0.10488301723750358\n","epoch 6/10 | Train Loss 5.102031618321007 | Test Loss 0.10129662661759811\n","epoch 7/10 | Train Loss 4.945302670120703 | Test Loss 0.09852589141975114\n","epoch 8/10 | Train Loss 4.821845125282588 | Test Loss 0.09629930141649723\n","epoch 9/10 | Train Loss 4.721201172450327 | Test Loss 0.09445695578184324\n","epoch 10/10 | Train Loss 4.637002894178037 | Test Loss 0.09289777158757297\n"]}],"source":["mini_batch_gd_weights = gradient_descent(\n","\tX_train, y_train,\n","\tX_train, y_train,\n","\tbatch_size=1024,\n","\tepochs=10,\n","\tlearning_rate=0.8,\n",")"]},{"cell_type":"markdown","id":"9d9d4ce7","metadata":{"papermill":{"duration":0.01947,"end_time":"2024-05-17T16:30:39.681611","exception":false,"start_time":"2024-05-17T16:30:39.662141","status":"completed"},"tags":[]},"source":["### Comparing Results"]},{"cell_type":"code","execution_count":22,"id":"be9cc01d","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:30:39.718613Z","iopub.status.busy":"2024-05-17T16:30:39.718198Z","iopub.status.idle":"2024-05-17T16:30:39.728119Z","shell.execute_reply":"2024-05-17T16:30:39.727046Z"},"papermill":{"duration":0.028212,"end_time":"2024-05-17T16:30:39.730276","exception":false,"start_time":"2024-05-17T16:30:39.702064","status":"completed"},"tags":[]},"outputs":[],"source":["def metrics(preds, y_true):\n","\n","\t# Calculate True Positives, False Positives, False Negatives\n","\ttp = np.sum((preds == 1) & (y_true == 1))\n","\tfp = np.sum((preds == 1) & (y_true == 0))\n","\tfn = np.sum((preds == 0) & (y_true == 1))\n","\t\n","\t# Calculate Accuracy\n","\taccuracy = np.sum(preds == y_true) / len(y_true)\n","\t\n","\t# Calculate Precision\n","\tprecision = tp / (tp + fp) if (tp + fp) != 0 else 0\n","\t\n","\t# Calculate Recall\n","\trecall = tp / (tp + fn) if (tp + fn) != 0 else 0\n","\t\n","\t# Calculate F1 Score\n","\tf1score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n","\t\n","\treturn accuracy, precision, recall, f1score\n","\n","def generate_report(preds, y_true):\n","\n","\tfor i in range(preds.shape[0]):\n","\t\tmax_index = np.argmax(preds[i])\n","\t\tpreds[i] = np.where(preds[i] == preds[i][max_index], 1, 0)\n","\n","\tresults = []\n","\tfor i in range(10):\n","\t\tresult = metrics(preds[:, i], y_true[:, i])\n","\t\tresults.append(result)\n","\treturn pd.DataFrame(\n","\t\tresults,\n","\t\tcolumns=['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n","\t\tindex=[f\"Digit {i}\" for i in range(10)]\n","\t)"]},{"cell_type":"code","execution_count":23,"id":"aff1733d","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:30:39.817573Z","iopub.status.busy":"2024-05-17T16:30:39.817166Z","iopub.status.idle":"2024-05-17T16:30:40.448228Z","shell.execute_reply":"2024-05-17T16:30:40.447111Z"},"papermill":{"duration":0.647007,"end_time":"2024-05-17T16:30:40.450702","exception":false,"start_time":"2024-05-17T16:30:39.803695","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Set Result on Batch Gradient Descent\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1 Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Digit 0</th>\n","      <td>0.96800</td>\n","      <td>0.830032</td>\n","      <td>0.849554</td>\n","      <td>0.839679</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 1</th>\n","      <td>0.96120</td>\n","      <td>0.780631</td>\n","      <td>0.915639</td>\n","      <td>0.842762</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 2</th>\n","      <td>0.93508</td>\n","      <td>0.727537</td>\n","      <td>0.554147</td>\n","      <td>0.629113</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 3</th>\n","      <td>0.93002</td>\n","      <td>0.633278</td>\n","      <td>0.746128</td>\n","      <td>0.685087</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 4</th>\n","      <td>0.92482</td>\n","      <td>0.632148</td>\n","      <td>0.541469</td>\n","      <td>0.583306</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 5</th>\n","      <td>0.91398</td>\n","      <td>0.712215</td>\n","      <td>0.076343</td>\n","      <td>0.137903</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 6</th>\n","      <td>0.96210</td>\n","      <td>0.807074</td>\n","      <td>0.811149</td>\n","      <td>0.809106</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 7</th>\n","      <td>0.95732</td>\n","      <td>0.798430</td>\n","      <td>0.786087</td>\n","      <td>0.792210</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 8</th>\n","      <td>0.89902</td>\n","      <td>0.485587</td>\n","      <td>0.720157</td>\n","      <td>0.580055</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 9</th>\n","      <td>0.90082</td>\n","      <td>0.502197</td>\n","      <td>0.664595</td>\n","      <td>0.572094</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Accuracy  Precision    Recall  F1 Score\n","Digit 0   0.96800   0.830032  0.849554  0.839679\n","Digit 1   0.96120   0.780631  0.915639  0.842762\n","Digit 2   0.93508   0.727537  0.554147  0.629113\n","Digit 3   0.93002   0.633278  0.746128  0.685087\n","Digit 4   0.92482   0.632148  0.541469  0.583306\n","Digit 5   0.91398   0.712215  0.076343  0.137903\n","Digit 6   0.96210   0.807074  0.811149  0.809106\n","Digit 7   0.95732   0.798430  0.786087  0.792210\n","Digit 8   0.89902   0.485587  0.720157  0.580055\n","Digit 9   0.90082   0.502197  0.664595  0.572094"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Train Set Result on Batch Gradient Descent\")\n","preds = logistic_regression_vector_prediction(batch_gd_weights, X_train)\n","generate_report(preds, y_train.values)"]},{"cell_type":"code","execution_count":24,"id":"0bd37bf1","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:30:40.477949Z","iopub.status.busy":"2024-05-17T16:30:40.477543Z","iopub.status.idle":"2024-05-17T16:30:40.770324Z","shell.execute_reply":"2024-05-17T16:30:40.769163Z"},"papermill":{"duration":0.309715,"end_time":"2024-05-17T16:30:40.77296","exception":false,"start_time":"2024-05-17T16:30:40.463245","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Set Result on Batch Gradient Descent\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1 Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Digit 0</th>\n","      <td>0.97040</td>\n","      <td>0.848761</td>\n","      <td>0.851344</td>\n","      <td>0.850051</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 1</th>\n","      <td>0.96580</td>\n","      <td>0.795322</td>\n","      <td>0.927694</td>\n","      <td>0.856423</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 2</th>\n","      <td>0.93435</td>\n","      <td>0.731246</td>\n","      <td>0.554402</td>\n","      <td>0.630661</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 3</th>\n","      <td>0.93620</td>\n","      <td>0.653537</td>\n","      <td>0.797059</td>\n","      <td>0.718198</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 4</th>\n","      <td>0.93055</td>\n","      <td>0.680905</td>\n","      <td>0.551654</td>\n","      <td>0.609502</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 5</th>\n","      <td>0.91460</td>\n","      <td>0.728111</td>\n","      <td>0.087438</td>\n","      <td>0.156126</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 6</th>\n","      <td>0.96610</td>\n","      <td>0.825587</td>\n","      <td>0.821299</td>\n","      <td>0.823438</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 7</th>\n","      <td>0.95990</td>\n","      <td>0.817261</td>\n","      <td>0.800283</td>\n","      <td>0.808683</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 8</th>\n","      <td>0.90375</td>\n","      <td>0.509857</td>\n","      <td>0.756430</td>\n","      <td>0.609137</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 9</th>\n","      <td>0.90955</td>\n","      <td>0.529848</td>\n","      <td>0.725381</td>\n","      <td>0.612385</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Accuracy  Precision    Recall  F1 Score\n","Digit 0   0.97040   0.848761  0.851344  0.850051\n","Digit 1   0.96580   0.795322  0.927694  0.856423\n","Digit 2   0.93435   0.731246  0.554402  0.630661\n","Digit 3   0.93620   0.653537  0.797059  0.718198\n","Digit 4   0.93055   0.680905  0.551654  0.609502\n","Digit 5   0.91460   0.728111  0.087438  0.156126\n","Digit 6   0.96610   0.825587  0.821299  0.823438\n","Digit 7   0.95990   0.817261  0.800283  0.808683\n","Digit 8   0.90375   0.509857  0.756430  0.609137\n","Digit 9   0.90955   0.529848  0.725381  0.612385"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Test Set Result on Batch Gradient Descent\")\n","preds = logistic_regression_vector_prediction(batch_gd_weights, X_test)\n","generate_report(preds, y_test.values)"]},{"cell_type":"code","execution_count":25,"id":"7f6e4136","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:30:40.801784Z","iopub.status.busy":"2024-05-17T16:30:40.801373Z","iopub.status.idle":"2024-05-17T16:30:41.426939Z","shell.execute_reply":"2024-05-17T16:30:41.425879Z"},"papermill":{"duration":0.642252,"end_time":"2024-05-17T16:30:41.429428","exception":false,"start_time":"2024-05-17T16:30:40.787176","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Set Result on Mini-Batch Gradient Descent\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1 Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Digit 0</th>\n","      <td>0.99058</td>\n","      <td>0.937439</td>\n","      <td>0.969181</td>\n","      <td>0.953046</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 1</th>\n","      <td>0.98896</td>\n","      <td>0.942966</td>\n","      <td>0.960902</td>\n","      <td>0.951849</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 2</th>\n","      <td>0.97728</td>\n","      <td>0.899666</td>\n","      <td>0.868156</td>\n","      <td>0.883630</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 3</th>\n","      <td>0.97406</td>\n","      <td>0.869895</td>\n","      <td>0.876887</td>\n","      <td>0.873377</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 4</th>\n","      <td>0.98128</td>\n","      <td>0.914957</td>\n","      <td>0.890101</td>\n","      <td>0.902358</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 5</th>\n","      <td>0.97224</td>\n","      <td>0.872254</td>\n","      <td>0.810697</td>\n","      <td>0.840350</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 6</th>\n","      <td>0.98814</td>\n","      <td>0.933201</td>\n","      <td>0.948091</td>\n","      <td>0.940587</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 7</th>\n","      <td>0.98300</td>\n","      <td>0.922942</td>\n","      <td>0.911884</td>\n","      <td>0.917379</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 8</th>\n","      <td>0.96782</td>\n","      <td>0.822462</td>\n","      <td>0.851508</td>\n","      <td>0.836733</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 9</th>\n","      <td>0.97168</td>\n","      <td>0.849648</td>\n","      <td>0.870088</td>\n","      <td>0.859746</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Accuracy  Precision    Recall  F1 Score\n","Digit 0   0.99058   0.937439  0.969181  0.953046\n","Digit 1   0.98896   0.942966  0.960902  0.951849\n","Digit 2   0.97728   0.899666  0.868156  0.883630\n","Digit 3   0.97406   0.869895  0.876887  0.873377\n","Digit 4   0.98128   0.914957  0.890101  0.902358\n","Digit 5   0.97224   0.872254  0.810697  0.840350\n","Digit 6   0.98814   0.933201  0.948091  0.940587\n","Digit 7   0.98300   0.922942  0.911884  0.917379\n","Digit 8   0.96782   0.822462  0.851508  0.836733\n","Digit 9   0.97168   0.849648  0.870088  0.859746"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Train Set Result on Mini-Batch Gradient Descent\")\n","preds = logistic_regression_vector_prediction(mini_batch_gd_weights, X_train)\n","generate_report(preds, y_train.values)"]},{"cell_type":"code","execution_count":26,"id":"afb99f6e","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:30:41.458278Z","iopub.status.busy":"2024-05-17T16:30:41.457862Z","iopub.status.idle":"2024-05-17T16:30:41.752493Z","shell.execute_reply":"2024-05-17T16:30:41.751361Z"},"papermill":{"duration":0.311591,"end_time":"2024-05-17T16:30:41.754725","exception":false,"start_time":"2024-05-17T16:30:41.443134","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Set Result on Mini-Batch Gradient Descent\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1 Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Digit 0</th>\n","      <td>0.99140</td>\n","      <td>0.943759</td>\n","      <td>0.970573</td>\n","      <td>0.956978</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 1</th>\n","      <td>0.99205</td>\n","      <td>0.960705</td>\n","      <td>0.967258</td>\n","      <td>0.963970</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 2</th>\n","      <td>0.98065</td>\n","      <td>0.921175</td>\n","      <td>0.884273</td>\n","      <td>0.902347</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 3</th>\n","      <td>0.97610</td>\n","      <td>0.871551</td>\n","      <td>0.898039</td>\n","      <td>0.884597</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 4</th>\n","      <td>0.98400</td>\n","      <td>0.929056</td>\n","      <td>0.906361</td>\n","      <td>0.917568</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 5</th>\n","      <td>0.97470</td>\n","      <td>0.894961</td>\n","      <td>0.815717</td>\n","      <td>0.853503</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 6</th>\n","      <td>0.98795</td>\n","      <td>0.928717</td>\n","      <td>0.947532</td>\n","      <td>0.938030</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 7</th>\n","      <td>0.98325</td>\n","      <td>0.923918</td>\n","      <td>0.917375</td>\n","      <td>0.920635</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 8</th>\n","      <td>0.96925</td>\n","      <td>0.830435</td>\n","      <td>0.866868</td>\n","      <td>0.848261</td>\n","    </tr>\n","    <tr>\n","      <th>Digit 9</th>\n","      <td>0.97435</td>\n","      <td>0.861538</td>\n","      <td>0.881218</td>\n","      <td>0.871267</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Accuracy  Precision    Recall  F1 Score\n","Digit 0   0.99140   0.943759  0.970573  0.956978\n","Digit 1   0.99205   0.960705  0.967258  0.963970\n","Digit 2   0.98065   0.921175  0.884273  0.902347\n","Digit 3   0.97610   0.871551  0.898039  0.884597\n","Digit 4   0.98400   0.929056  0.906361  0.917568\n","Digit 5   0.97470   0.894961  0.815717  0.853503\n","Digit 6   0.98795   0.928717  0.947532  0.938030\n","Digit 7   0.98325   0.923918  0.917375  0.920635\n","Digit 8   0.96925   0.830435  0.866868  0.848261\n","Digit 9   0.97435   0.861538  0.881218  0.871267"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Test Set Result on Mini-Batch Gradient Descent\")\n","preds = logistic_regression_vector_prediction(mini_batch_gd_weights, X_test)\n","generate_report(preds, y_test.values)"]},{"cell_type":"markdown","id":"597e8245","metadata":{"papermill":{"duration":0.013399,"end_time":"2024-05-17T16:30:41.781659","exception":false,"start_time":"2024-05-17T16:30:41.76826","status":"completed"},"tags":[]},"source":["# 4. Iris Dataset\n","In this section we will apply Logistic Regression functions implemented from scracth above, on Iris Dataset"]},{"cell_type":"code","execution_count":27,"id":"c476db5b","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:30:41.810512Z","iopub.status.busy":"2024-05-17T16:30:41.809801Z","iopub.status.idle":"2024-05-17T16:31:27.939712Z","shell.execute_reply":"2024-05-17T16:31:27.938528Z"},"papermill":{"duration":46.160936,"end_time":"2024-05-17T16:31:27.956033","exception":false,"start_time":"2024-05-17T16:30:41.795097","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train.shape: (120, 4)\n","y_train.shape: (120, 3)\n","X_test.shape: (30, 4)\n","y_test.shape: (30, 3)\n"]}],"source":["from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","\n","# Load the Iris dataset\n","iris = fetch_openml('iris', version=1, cache=True, parser='auto')\n","\n","# Pandas data frame with feature vectors\n","X = iris.data\n","\n","# Scale\n","# X = X / 255.\n","\n","# Labels\n","y = iris.target\n","\n","# one hot encoding\n","y = pd.get_dummies(y).astype(int)\n","\n","# train test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n","\n","# show shapes\n","print(\"X_train.shape:\", X_train.shape)\n","print(\"y_train.shape:\", y_train.shape)\n","print(\"X_test.shape:\", X_test.shape)\n","print(\"y_test.shape:\", y_test.shape)"]},{"cell_type":"markdown","id":"454040c3","metadata":{"papermill":{"duration":0.013413,"end_time":"2024-05-17T16:31:27.982964","exception":false,"start_time":"2024-05-17T16:31:27.969551","status":"completed"},"tags":[]},"source":["### Applying Gradient Descents"]},{"cell_type":"code","execution_count":28,"id":"e0ba12bc","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:28.011593Z","iopub.status.busy":"2024-05-17T16:31:28.011198Z","iopub.status.idle":"2024-05-17T16:31:28.043185Z","shell.execute_reply":"2024-05-17T16:31:28.04229Z"},"papermill":{"duration":0.049057,"end_time":"2024-05-17T16:31:28.045402","exception":false,"start_time":"2024-05-17T16:31:27.996345","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 1/10 | Train Loss 4.726550872712751 | Test Loss 3.413371548941273\n","epoch 2/10 | Train Loss 3.413371548941273 | Test Loss 2.1997866006835736\n","epoch 3/10 | Train Loss 2.1997866006835736 | Test Loss 1.3460407827657273\n","epoch 4/10 | Train Loss 1.3460407827657273 | Test Loss 0.9297036782711285\n","epoch 5/10 | Train Loss 0.9297036782711285 | Test Loss 0.7103046560803397\n","epoch 6/10 | Train Loss 0.7103046560803397 | Test Loss 0.6459606864066962\n","epoch 7/10 | Train Loss 0.6459606864066962 | Test Loss 0.630006859570538\n","epoch 8/10 | Train Loss 0.630006859570538 | Test Loss 0.6184016298020568\n","epoch 9/10 | Train Loss 0.6184016298020568 | Test Loss 0.6075446578667751\n","epoch 10/10 | Train Loss 0.6075446578667751 | Test Loss 0.597184384659209\n"]}],"source":["batch_gd_weights = gradient_descent(\n","\tX_train, y_train,\n","\tX_train, y_train,\n","\tbatch_size=len(X_train),\n","\tepochs=10,\n","\tlearning_rate=0.05,\n",")"]},{"cell_type":"code","execution_count":29,"id":"41ff3252","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:28.074958Z","iopub.status.busy":"2024-05-17T16:31:28.074577Z","iopub.status.idle":"2024-05-17T16:31:28.10549Z","shell.execute_reply":"2024-05-17T16:31:28.104291Z"},"papermill":{"duration":0.048496,"end_time":"2024-05-17T16:31:28.107837","exception":false,"start_time":"2024-05-17T16:31:28.059341","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 1/10 | Train Loss 4.96391702489201 | Test Loss 3.749646611340176\n","epoch 2/10 | Train Loss 3.749646611340176 | Test Loss 2.746347980902355\n","epoch 3/10 | Train Loss 2.746347980902355 | Test Loss 1.9193740623534756\n","epoch 4/10 | Train Loss 1.9193740623534756 | Test Loss 1.2705278029138203\n","epoch 5/10 | Train Loss 1.2705278029138203 | Test Loss 0.841395547844468\n","epoch 6/10 | Train Loss 0.841395547844468 | Test Loss 0.6754446330317418\n","epoch 7/10 | Train Loss 0.6754446330317418 | Test Loss 0.6474395423501708\n","epoch 8/10 | Train Loss 0.6474395423501708 | Test Loss 0.6348320634575362\n","epoch 9/10 | Train Loss 0.6348320634575362 | Test Loss 0.6240264513240896\n","epoch 10/10 | Train Loss 0.6240264513240896 | Test Loss 0.6138045076813763\n"]}],"source":["mini_batch_gd_weights = gradient_descent(\n","\tX_train, y_train,\n","\tX_train, y_train,\n","\tbatch_size=1024,\n","\tepochs=10,\n","\tlearning_rate=0.05,\n",")"]},{"cell_type":"markdown","id":"290468c2","metadata":{"papermill":{"duration":0.013466,"end_time":"2024-05-17T16:31:28.135352","exception":false,"start_time":"2024-05-17T16:31:28.121886","status":"completed"},"tags":[]},"source":["### Comparing Results"]},{"cell_type":"code","execution_count":30,"id":"ac3ba26d","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:28.164711Z","iopub.status.busy":"2024-05-17T16:31:28.164287Z","iopub.status.idle":"2024-05-17T16:31:28.174457Z","shell.execute_reply":"2024-05-17T16:31:28.17325Z"},"papermill":{"duration":0.027348,"end_time":"2024-05-17T16:31:28.176555","exception":false,"start_time":"2024-05-17T16:31:28.149207","status":"completed"},"tags":[]},"outputs":[],"source":["def metrics(preds, y_true):\n","\n","\t# Calculate True Positives, False Positives, False Negatives\n","\ttp = np.sum((preds == 1) & (y_true == 1))\n","\tfp = np.sum((preds == 1) & (y_true == 0))\n","\tfn = np.sum((preds == 0) & (y_true == 1))\n","\t\n","\t# Calculate Accuracy\n","\taccuracy = np.sum(preds == y_true) / len(y_true)\n","\t\n","\t# Calculate Precision\n","\tprecision = tp / (tp + fp) if (tp + fp) != 0 else 0\n","\t\n","\t# Calculate Recall\n","\trecall = tp / (tp + fn) if (tp + fn) != 0 else 0\n","\t\n","\t# Calculate F1 Score\n","\tf1score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n","\t\n","\treturn accuracy, precision, recall, f1score\n","\n","def generate_report(preds, y_true):\n","\n","\tfor i in range(preds.shape[0]):\n","\t\tmax_index = np.argmax(preds[i])\n","\t\tpreds[i] = np.where(preds[i] == preds[i][max_index], 1, 0)\n","\n","\tresults = []\n","\tfor i in range(3):\n","\t\tresult = metrics(preds[:, i], y_true[:, i])\n","\t\tresults.append(result)\n","\treturn pd.DataFrame(\n","\t\tresults,\n","\t\tcolumns=['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n","\t\tindex=[f\"Category {i}\" for i in range(3)]\n","\t)"]},{"cell_type":"code","execution_count":31,"id":"06f1f567","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:28.207487Z","iopub.status.busy":"2024-05-17T16:31:28.207072Z","iopub.status.idle":"2024-05-17T16:31:28.222498Z","shell.execute_reply":"2024-05-17T16:31:28.221408Z"},"papermill":{"duration":0.033749,"end_time":"2024-05-17T16:31:28.224614","exception":false,"start_time":"2024-05-17T16:31:28.190865","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Set Result on Batch Gradient Descent\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1 Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Category 0</th>\n","      <td>0.666667</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>Category 1</th>\n","      <td>0.350000</td>\n","      <td>0.025</td>\n","      <td>0.025</td>\n","      <td>0.025000</td>\n","    </tr>\n","    <tr>\n","      <th>Category 2</th>\n","      <td>0.666667</td>\n","      <td>0.500</td>\n","      <td>1.000</td>\n","      <td>0.666667</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            Accuracy  Precision  Recall  F1 Score\n","Category 0  0.666667      0.000   0.000  0.000000\n","Category 1  0.350000      0.025   0.025  0.025000\n","Category 2  0.666667      0.500   1.000  0.666667"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Train Set Result on Batch Gradient Descent\")\n","preds = logistic_regression_vector_prediction(batch_gd_weights, X_train)\n","generate_report(preds, y_train.values)"]},{"cell_type":"code","execution_count":32,"id":"1a487ed9","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:28.25536Z","iopub.status.busy":"2024-05-17T16:31:28.254898Z","iopub.status.idle":"2024-05-17T16:31:28.270511Z","shell.execute_reply":"2024-05-17T16:31:28.269389Z"},"papermill":{"duration":0.03332,"end_time":"2024-05-17T16:31:28.272497","exception":false,"start_time":"2024-05-17T16:31:28.239177","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Set Result on Batch Gradient Descent\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1 Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Category 0</th>\n","      <td>0.666667</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>Category 1</th>\n","      <td>0.333333</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>Category 2</th>\n","      <td>0.666667</td>\n","      <td>0.5</td>\n","      <td>1.0</td>\n","      <td>0.666667</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            Accuracy  Precision  Recall  F1 Score\n","Category 0  0.666667        0.0     0.0  0.000000\n","Category 1  0.333333        0.0     0.0  0.000000\n","Category 2  0.666667        0.5     1.0  0.666667"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Test Set Result on Batch Gradient Descent\")\n","preds = logistic_regression_vector_prediction(batch_gd_weights, X_test)\n","generate_report(preds, y_test.values)"]},{"cell_type":"code","execution_count":33,"id":"b131ea0b","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:28.304513Z","iopub.status.busy":"2024-05-17T16:31:28.303297Z","iopub.status.idle":"2024-05-17T16:31:28.319076Z","shell.execute_reply":"2024-05-17T16:31:28.317753Z"},"papermill":{"duration":0.033831,"end_time":"2024-05-17T16:31:28.321254","exception":false,"start_time":"2024-05-17T16:31:28.287423","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Set Result on Mini-Batch Gradient Descent\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1 Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Category 0</th>\n","      <td>0.666667</td>\n","      <td>0.0000</td>\n","      <td>0.000</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>Category 1</th>\n","      <td>0.333333</td>\n","      <td>0.0000</td>\n","      <td>0.000</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>Category 2</th>\n","      <td>0.650000</td>\n","      <td>0.4875</td>\n","      <td>0.975</td>\n","      <td>0.65</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            Accuracy  Precision  Recall  F1 Score\n","Category 0  0.666667     0.0000   0.000      0.00\n","Category 1  0.333333     0.0000   0.000      0.00\n","Category 2  0.650000     0.4875   0.975      0.65"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Train Set Result on Mini-Batch Gradient Descent\")\n","preds = logistic_regression_vector_prediction(mini_batch_gd_weights, X_train)\n","generate_report(preds, y_train.values)"]},{"cell_type":"code","execution_count":34,"id":"db028226","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:28.352557Z","iopub.status.busy":"2024-05-17T16:31:28.352139Z","iopub.status.idle":"2024-05-17T16:31:28.366109Z","shell.execute_reply":"2024-05-17T16:31:28.365239Z"},"papermill":{"duration":0.031943,"end_time":"2024-05-17T16:31:28.368014","exception":false,"start_time":"2024-05-17T16:31:28.336071","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Set Result on Mini-Batch Gradient Descent\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1 Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Category 0</th>\n","      <td>0.666667</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>Category 1</th>\n","      <td>0.366667</td>\n","      <td>0.090909</td>\n","      <td>0.1</td>\n","      <td>0.095238</td>\n","    </tr>\n","    <tr>\n","      <th>Category 2</th>\n","      <td>0.700000</td>\n","      <td>0.526316</td>\n","      <td>1.0</td>\n","      <td>0.689655</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            Accuracy  Precision  Recall  F1 Score\n","Category 0  0.666667   0.000000     0.0  0.000000\n","Category 1  0.366667   0.090909     0.1  0.095238\n","Category 2  0.700000   0.526316     1.0  0.689655"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Test Set Result on Mini-Batch Gradient Descent\")\n","preds = logistic_regression_vector_prediction(mini_batch_gd_weights, X_test)\n","generate_report(preds, y_test.values)"]},{"cell_type":"code","execution_count":null,"id":"a186f80f","metadata":{"papermill":{"duration":0.014761,"end_time":"2024-05-17T16:31:28.397863","exception":false,"start_time":"2024-05-17T16:31:28.383102","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"e71d8934","metadata":{"papermill":{"duration":0.014882,"end_time":"2024-05-17T16:31:28.427881","exception":false,"start_time":"2024-05-17T16:31:28.412999","status":"completed"},"tags":[]},"source":["**Find More Labs**\n","\n","This lab is from my Machine Learning Course, that is a part of my [Software Engineering](https://seecs.nust.edu.pk/program/bachelor-of-software-engineering-for-fall-2021-onward) Degree at [NUST](https://nust.edu.pk).\n","\n","The content in the provided list of notebooks covers a range of topics in **machine learning** and **data analysis** implemented from scratch or using popular libraries like **NumPy**, **pandas**, **scikit-learn**, **seaborn**, and **matplotlib**. It includes introductory materials on NumPy showcasing its efficiency for mathematical operations, **linear regression**, **logistic regression**, **decision trees**, **K-nearest neighbors (KNN)**, **support vector machines (SVM)**, **Naive Bayes**, **K-means** clustering, principle component analysis (**PCA**), and **neural networks** with **backpropagation**. Each notebook demonstrates practical implementation and application of these algorithms on various datasets such as the **California Housing** Dataset, **MNIST** dataset, **Iris** dataset, **Auto-MPG** dataset, and the **UCI Adult Census Income** dataset. Additionally, it covers topics like **gradient descent optimization**, model evaluation metrics (e.g., **accuracy, precision, recall, f1 score**), **regularization** techniques (e.g., **Lasso**, **Ridge**), and **data visualization**.\n","\n","| Title                                                                                                                   | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n","| ----------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n","| [01 - Intro to Numpy](https://www.kaggle.com/code/sacrum/ml-labs-01-intro-to-numpy)                                     | The notebook demonstrates NumPy's efficiency for mathematical operations like array `reshaping`, `sigmoid`, `softmax`, `dot` and `outer products`, `L1 and L2 losses`, and matrix operations. It highlights NumPy's superiority over standard Python lists in speed and convenience for scientific computing and machine learning tasks.                                                                                                                                                                                              |\n","| [02 - Linear Regression From Scratch](https://www.kaggle.com/code/sacrum/ml-labs-02-linear-regression-from-scratch)     | This notebook implements `linear regression` and `gradient descent` from scratch in Python using `NumPy`, focusing on predicting house prices with the `California Housing Dataset`. It defines functions for prediction, `MSE` calculation, and gradient computation. Batch gradient descent is used for optimization. The dataset is loaded, scaled, and split. `Batch, stochastic, and mini-batch gradient descents` are applied with varying hyperparameters. Finally, the MSEs of the predictions from each method are compared. |\n","| [03 - Logistic Regression from Scratch](https://www.kaggle.com/code/sacrum/ml-labs-03-logistic-regression-from-scratch) | This notebook outlines the implementation of `logistic regression` from scratch in Python using `NumPy`, including functions for prediction, loss calculation, gradient computation, and batch `gradient descent` optimization, applied to the `MNIST` dataset for handwritten digit recognition and `Iris` data. And also inclues metrics like `accuracy`, `precision`, `recall`, `f1 score`                                                                                                                                         |\n","| [04 - Auto-MPG Regression](https://www.kaggle.com/code/sacrum/ml-labs-04-auto-mpg-regression)                           | The notebook uses `pandas` for data manipulation, `seaborn` and `matplotlib` for visualization, and `sklearn` for `linear regression` and `regularization` techniques (`Lasso` and `Ridge`). It includes data loading, processing, visualization, model training, and evaluation on the `Auto-MPG dataset`.                                                                                                                                                                                                                           |\n","| [05 - Desicion Trees from Scratch](https://www.kaggle.com/code/sacrum/ml-labs-05-desicion-trees-from-scratch)           | In this notebook, `DecisionTree` algorithm has been implmented from scratch and applied on dummy dataset                                                                                                                                                                                                                                                                                                                                                                                                                              |\n","| [06 - KNN from Scratch](https://www.kaggle.com/code/sacrum/ml-labs-06-knn-from-scratch)                                 | In this notebook, `K-Nearest Neighbour` algorithm has been implemented from scratch and compared with KNN provided in scikit-learn package                                                                                                                                                                                                                                                                                                                                                                                            |\n","| [07 - SVM](https://www.kaggle.com/code/sacrum/ml-labs-07-svm)                                                           | This notebook implements `SVM classifier` on `Iris Dataset`                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n","| [08 - Naive Bayes](https://www.kaggle.com/code/sacrum/ml-labs-08-naive-bayes)                                           | This notebook trains `Naive Bayes` and compares it with other algorithms `Decision Trees`, `SVM` and `Logistic Regression`                                                                                                                                                                                                                                                                                                                                                                                                            |\n","| [09 - K-means](https://www.kaggle.com/code/sacrum/ml-labs-09-k-means)                                                   | In this notebook `K-means` algorithm has been implemented using `scikit-learn` and different values of `k` are compared to understand the `elbow method` in `Calinski Harabasz Scores`                                                                                                                                                                                                                                                                                                                                                |\n","| [10 - UCI Adult Census Income](https://www.kaggle.com/code/sacrum/ml-labs-10-uci-adult-census-income)                   | Here I have used the UCI Adult Income dataset and applied different machine learning algorithms to find the best model configuration for predicting salary from the given information                                                                                                                                                                                                                                                                                                                                                 |\n","| [11 - PCA](https://www.kaggle.com/code/sacrum/ml-labs-11-pca)                                                           | `Principle Component Analysis` implemented from scratch                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n","| [12 - Neural Networks](https://www.kaggle.com/code/sacrum/ml-labs-12-neural-networks)                                   | This code implements neural networks with back propagation from scratch                                                                                                                                                                                                                                                                                                                                                                                                                                                               |"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":19,"sourceId":420,"sourceType":"datasetVersion"},{"datasetId":102285,"sourceId":242592,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"ai","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":164.018921,"end_time":"2024-05-17T16:31:29.064658","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-17T16:28:45.045737","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}