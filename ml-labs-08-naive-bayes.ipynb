{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sacrum/ml-labs-08-naive-bayes?scriptVersionId=178243645\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"7f91a873","metadata":{"id":"hrI1XeiP9AS8","papermill":{"duration":0.004479,"end_time":"2024-05-17T16:29:55.309819","exception":false,"start_time":"2024-05-17T16:29:55.30534","status":"completed"},"tags":[]},"source":["Train a Gaussian Na誰ve Bayes classifier on the MNIST digits dataset. Compare your results with classifiers including Decision Trees, Support Vector Classifier and Logistic Regression Classifier. Identify which technique performs best on the digits dataset and explain why the performance of this technique is better than all the others."]},{"cell_type":"code","execution_count":1,"id":"3ed9a866","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:29:55.320347Z","iopub.status.busy":"2024-05-17T16:29:55.319325Z","iopub.status.idle":"2024-05-17T16:29:56.196935Z","shell.execute_reply":"2024-05-17T16:29:56.195864Z"},"id":"Jf5M8hD69ATI","papermill":{"duration":0.885545,"end_time":"2024-05-17T16:29:56.199532","exception":false,"start_time":"2024-05-17T16:29:55.313987","status":"completed"},"tags":[]},"outputs":[],"source":["# Import Necessary Libraries\n","\n","import pandas as pd\n","import numpy as np\n","import time\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","id":"64ac0d86","metadata":{"id":"odxue1KR9ATJ","papermill":{"duration":0.003823,"end_time":"2024-05-17T16:29:56.209638","exception":false,"start_time":"2024-05-17T16:29:56.205815","status":"completed"},"tags":[]},"source":["# Data"]},{"cell_type":"code","execution_count":2,"id":"be975cb7","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:29:56.220294Z","iopub.status.busy":"2024-05-17T16:29:56.219737Z","iopub.status.idle":"2024-05-17T16:30:56.98306Z","shell.execute_reply":"2024-05-17T16:30:56.982195Z"},"id":"-G-GrHUF9ATK","papermill":{"duration":60.772243,"end_time":"2024-05-17T16:30:56.985851","exception":false,"start_time":"2024-05-17T16:29:56.213608","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.datasets import fetch_openml\n","\n","# Load the MNIST dataset\n","mnist = fetch_openml('mnist_784', version=1, cache=True, parser='auto')\n","\n","# Pandas data frame with feature vectors\n","X = mnist.data\n","\n","# Scale pixel values\n","X = X / 255.\n","\n","# Labels\n","y = mnist.target\n","\n","# Use sample of Data\n","N = -1\n","X = X[:N]\n","y = y[:N]"]},{"cell_type":"code","execution_count":3,"id":"1e366980","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:30:56.996097Z","iopub.status.busy":"2024-05-17T16:30:56.995654Z","iopub.status.idle":"2024-05-17T16:30:57.593646Z","shell.execute_reply":"2024-05-17T16:30:57.592191Z"},"id":"aKdSw7Ib9ATK","outputId":"54685b3b-0a95-4584-f4be-61ab8af409ee","papermill":{"duration":0.605758,"end_time":"2024-05-17T16:30:57.596119","exception":false,"start_time":"2024-05-17T16:30:56.990361","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train.shape: (55999, 784)\n","y_train.shape: (55999,)\n","X_test.shape: (14000, 784)\n","y_test.shape: (14000,)\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","# train test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","\tX,\n","\ty,\n","\ttest_size=0.2,\n","\tstratify=y\n",")\n","\n","# show shapes\n","print(\"X_train.shape:\", X_train.shape)\n","print(\"y_train.shape:\", y_train.shape)\n","print(\"X_test.shape:\", X_test.shape)\n","print(\"y_test.shape:\", y_test.shape)"]},{"cell_type":"markdown","id":"54a8b6af","metadata":{"id":"ea834Wkq9ATL","papermill":{"duration":0.003971,"end_time":"2024-05-17T16:30:57.604416","exception":false,"start_time":"2024-05-17T16:30:57.600445","status":"completed"},"tags":[]},"source":["# Model"]},{"cell_type":"code","execution_count":4,"id":"a2de7e21","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:30:57.615162Z","iopub.status.busy":"2024-05-17T16:30:57.614397Z","iopub.status.idle":"2024-05-17T16:30:57.766836Z","shell.execute_reply":"2024-05-17T16:30:57.765955Z"},"id":"4Yr3WbhS9ATM","papermill":{"duration":0.160505,"end_time":"2024-05-17T16:30:57.769386","exception":false,"start_time":"2024-05-17T16:30:57.608881","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import classification_report\n","\n","# Initializing Models\n","models = [\n","\tDecisionTreeClassifier(),\n","\tSVC(),\n","\tLogisticRegression(),\n","\tGaussianNB(),\n","]"]},{"cell_type":"code","execution_count":5,"id":"4ca008bc","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:30:57.77999Z","iopub.status.busy":"2024-05-17T16:30:57.779592Z","iopub.status.idle":"2024-05-17T16:37:57.271886Z","shell.execute_reply":"2024-05-17T16:37:57.270044Z"},"id":"zqf-z4Lv9ATM","outputId":"8c1573c5-86cf-458f-d074-0041bb757d4f","papermill":{"duration":419.50038,"end_time":"2024-05-17T16:37:57.274329","exception":false,"start_time":"2024-05-17T16:30:57.773949","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":[">>> DecisionTreeClassifier\n","    Training Time: 21.416 seconds\n","    Prediction Time: 0.036 seconds\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.92      0.92      1381\n","           1       0.94      0.96      0.95      1575\n","           2       0.86      0.84      0.85      1398\n","           3       0.83      0.83      0.83      1428\n","           4       0.89      0.88      0.89      1365\n","           5       0.82      0.82      0.82      1262\n","           6       0.89      0.90      0.89      1375\n","           7       0.90      0.90      0.90      1459\n","           8       0.83      0.81      0.82      1365\n","           9       0.83      0.85      0.84      1392\n","\n","    accuracy                           0.87     14000\n","   macro avg       0.87      0.87      0.87     14000\n","weighted avg       0.87      0.87      0.87     14000\n","\n","\n",">>> SVC\n","    Training Time: 228.699 seconds\n","    Prediction Time: 150.132 seconds\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.99      0.99      1381\n","           1       0.99      0.99      0.99      1575\n","           2       0.98      0.98      0.98      1398\n","           3       0.98      0.97      0.97      1428\n","           4       0.97      0.98      0.97      1365\n","           5       0.97      0.98      0.98      1262\n","           6       0.99      0.99      0.99      1375\n","           7       0.97      0.98      0.98      1459\n","           8       0.98      0.97      0.97      1365\n","           9       0.97      0.96      0.97      1392\n","\n","    accuracy                           0.98     14000\n","   macro avg       0.98      0.98      0.98     14000\n","weighted avg       0.98      0.98      0.98     14000\n","\n","\n",">>> LogisticRegression\n","    Training Time: 14.531 seconds\n","    Prediction Time: 0.066 seconds\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.97      0.96      1381\n","           1       0.95      0.97      0.96      1575\n","           2       0.92      0.90      0.91      1398\n","           3       0.90      0.88      0.89      1428\n","           4       0.93      0.93      0.93      1365\n","           5       0.89      0.89      0.89      1262\n","           6       0.94      0.96      0.95      1375\n","           7       0.93      0.93      0.93      1459\n","           8       0.89      0.88      0.89      1365\n","           9       0.89      0.90      0.89      1392\n","\n","    accuracy                           0.92     14000\n","   macro avg       0.92      0.92      0.92     14000\n","weighted avg       0.92      0.92      0.92     14000\n","\n","\n",">>> GaussianNB\n","    Training Time: 1.064 seconds\n","    Prediction Time: 0.759 seconds\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.72      0.90      0.80      1381\n","           1       0.78      0.93      0.85      1575\n","           2       0.86      0.29      0.44      1398\n","           3       0.73      0.31      0.44      1428\n","           4       0.80      0.17      0.28      1365\n","           5       0.59      0.05      0.08      1262\n","           6       0.61      0.95      0.75      1375\n","           7       0.89      0.29      0.44      1459\n","           8       0.30      0.64      0.41      1365\n","           9       0.38      0.94      0.54      1392\n","\n","    accuracy                           0.55     14000\n","   macro avg       0.67      0.55      0.50     14000\n","weighted avg       0.67      0.55      0.51     14000\n","\n","\n"]}],"source":["results = {}\n","\n","for model in models:\n","\tprint(\">>>\", model.__class__.__name__)\n","\n","\t# Fit on train data\n","\ti = time.time()\n","\tmodel.fit(X_train, y_train)\n","\tprint(f\"    Training Time: {round(time.time() - i, 3)} seconds\")\n","\n","\t# get predictions\n","\ti = time.time()\n","\tpreds = model.predict(X_test)\n","\tprint(f\"    Prediction Time: {round(time.time() - i, 3)} seconds\")\n","\n","\t# get report\n","\treport = classification_report(y_test, preds)\n","\n","\tprint()\n","\tprint(report)\n","\tprint()\n"]},{"cell_type":"markdown","id":"3d1e6d57","metadata":{"id":"HTwLoHnJ9ATN","papermill":{"duration":0.005045,"end_time":"2024-05-17T16:37:57.284871","exception":false,"start_time":"2024-05-17T16:37:57.279826","status":"completed"},"tags":[]},"source":["# Results\n","\n","| Classifier             | Accuracy | Precision | Recall | F1-Score | Training Time (s) | Prediction Time (s) |\n","|------------------------|----------|-----------|--------|----------|--------------------|----------------------|\n","| DecisionTreeClassifier | 0.87     | 0.87      | 0.87   | 0.87     | 17.882             | 0.03                 |\n","| Support Vector Classifier (SVC) | 0.98     | 0.98      | 0.98   | 0.98     | 206.083            | 127.571              |\n","| Logistic Regression    | 0.92     | 0.92      | 0.92   | 0.92     | 26.421             | 0.063                |\n","| Gaussian Na誰ve Bayes   | 0.56     | 0.69      | 0.55   | 0.51     | 0.975              | 0.691                |\n","\n","The Support Vector Classifier (SVC) still outperforms the other classifiers on the MNIST digits dataset, achieving an accuracy of 98%. This high accuracy is due to its ability to find optimal separating hyperplanes in high-dimensional spaces, which is well-suited for classifying the complex and non-linear relationships in the MNIST dataset.\n","\n","In comparison, the DecisionTreeClassifier, LogisticRegression, and Gaussian Na誰ve Bayes classifiers have lower accuracies. The DecisionTreeClassifier suffers from potential overfitting, LogisticRegression may struggle with non-linear relationships, and Gaussian Na誰ve Bayes assumes independence between features, which may not hold true for the pixel values of the digits in MNIST.\n","\n","Overall, the Support Vector Classifier (SVC) remains the best performing classifier on the MNIST digits dataset due to its ability to handle high-dimensional data and find optimal separating hyperplanes, leading to superior classification performance."]},{"cell_type":"code","execution_count":null,"id":"cf4b9b4c","metadata":{"papermill":{"duration":0.004864,"end_time":"2024-05-17T16:37:57.295034","exception":false,"start_time":"2024-05-17T16:37:57.29017","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"065ef8e8","metadata":{"papermill":{"duration":0.00505,"end_time":"2024-05-17T16:37:57.305397","exception":false,"start_time":"2024-05-17T16:37:57.300347","status":"completed"},"tags":[]},"source":["**Find More Labs**\n","\n","This lab is from my Machine Learning Course, that is a part of my [Software Engineering](https://seecs.nust.edu.pk/program/bachelor-of-software-engineering-for-fall-2021-onward) Degree at [NUST](https://nust.edu.pk).\n","\n","The content in the provided list of notebooks covers a range of topics in **machine learning** and **data analysis** implemented from scratch or using popular libraries like **NumPy**, **pandas**, **scikit-learn**, **seaborn**, and **matplotlib**. It includes introductory materials on NumPy showcasing its efficiency for mathematical operations, **linear regression**, **logistic regression**, **decision trees**, **K-nearest neighbors (KNN)**, **support vector machines (SVM)**, **Naive Bayes**, **K-means** clustering, principle component analysis (**PCA**), and **neural networks** with **backpropagation**. Each notebook demonstrates practical implementation and application of these algorithms on various datasets such as the **California Housing** Dataset, **MNIST** dataset, **Iris** dataset, **Auto-MPG** dataset, and the **UCI Adult Census Income** dataset. Additionally, it covers topics like **gradient descent optimization**, model evaluation metrics (e.g., **accuracy, precision, recall, f1 score**), **regularization** techniques (e.g., **Lasso**, **Ridge**), and **data visualization**.\n","\n","| Title                                                                                                                   | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n","| ----------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n","| [01 - Intro to Numpy](https://www.kaggle.com/code/sacrum/ml-labs-01-intro-to-numpy)                                     | The notebook demonstrates NumPy's efficiency for mathematical operations like array `reshaping`, `sigmoid`, `softmax`, `dot` and `outer products`, `L1 and L2 losses`, and matrix operations. It highlights NumPy's superiority over standard Python lists in speed and convenience for scientific computing and machine learning tasks.                                                                                                                                                                                              |\n","| [02 - Linear Regression From Scratch](https://www.kaggle.com/code/sacrum/ml-labs-02-linear-regression-from-scratch)     | This notebook implements `linear regression` and `gradient descent` from scratch in Python using `NumPy`, focusing on predicting house prices with the `California Housing Dataset`. It defines functions for prediction, `MSE` calculation, and gradient computation. Batch gradient descent is used for optimization. The dataset is loaded, scaled, and split. `Batch, stochastic, and mini-batch gradient descents` are applied with varying hyperparameters. Finally, the MSEs of the predictions from each method are compared. |\n","| [03 - Logistic Regression from Scratch](https://www.kaggle.com/code/sacrum/ml-labs-03-logistic-regression-from-scratch) | This notebook outlines the implementation of `logistic regression` from scratch in Python using `NumPy`, including functions for prediction, loss calculation, gradient computation, and batch `gradient descent` optimization, applied to the `MNIST` dataset for handwritten digit recognition and `Iris` data. And also inclues metrics like `accuracy`, `precision`, `recall`, `f1 score`                                                                                                                                         |\n","| [04 - Auto-MPG Regression](https://www.kaggle.com/code/sacrum/ml-labs-04-auto-mpg-regression)                           | The notebook uses `pandas` for data manipulation, `seaborn` and `matplotlib` for visualization, and `sklearn` for `linear regression` and `regularization` techniques (`Lasso` and `Ridge`). It includes data loading, processing, visualization, model training, and evaluation on the `Auto-MPG dataset`.                                                                                                                                                                                                                           |\n","| [05 - Desicion Trees from Scratch](https://www.kaggle.com/code/sacrum/ml-labs-05-desicion-trees-from-scratch)           | In this notebook, `DecisionTree` algorithm has been implmented from scratch and applied on dummy dataset                                                                                                                                                                                                                                                                                                                                                                                                                              |\n","| [06 - KNN from Scratch](https://www.kaggle.com/code/sacrum/ml-labs-06-knn-from-scratch)                                 | In this notebook, `K-Nearest Neighbour` algorithm has been implemented from scratch and compared with KNN provided in scikit-learn package                                                                                                                                                                                                                                                                                                                                                                                            |\n","| [07 - SVM](https://www.kaggle.com/code/sacrum/ml-labs-07-svm)                                                           | This notebook implements `SVM classifier` on `Iris Dataset`                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n","| [08 - Naive Bayes](https://www.kaggle.com/code/sacrum/ml-labs-08-naive-bayes)                                           | This notebook trains `Naive Bayes` and compares it with other algorithms `Decision Trees`, `SVM` and `Logistic Regression`                                                                                                                                                                                                                                                                                                                                                                                                            |\n","| [09 - K-means](https://www.kaggle.com/code/sacrum/ml-labs-09-k-means)                                                   | In this notebook `K-means` algorithm has been implemented using `scikit-learn` and different values of `k` are compared to understand the `elbow method` in `Calinski Harabasz Scores`                                                                                                                                                                                                                                                                                                                                                |\n","| [10 - UCI Adult Census Income](https://www.kaggle.com/code/sacrum/ml-labs-10-uci-adult-census-income)                   | Here I have used the UCI Adult Income dataset and applied different machine learning algorithms to find the best model configuration for predicting salary from the given information                                                                                                                                                                                                                                                                                                                                                 |\n","| [11 - PCA](https://www.kaggle.com/code/sacrum/ml-labs-11-pca)                                                           | `Principle Component Analysis` implemented from scratch                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n","| [12 - Neural Networks](https://www.kaggle.com/code/sacrum/ml-labs-12-neural-networks)                                   | This code implements neural networks with back propagation from scratch                                                                                                                                                                                                                                                                                                                                                                                                                                                               |"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":102285,"sourceId":242592,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":485.560172,"end_time":"2024-05-17T16:37:57.934652","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-17T16:29:52.37448","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}