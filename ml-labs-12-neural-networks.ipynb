{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sacrum/ml-labs-12-neural-networks?scriptVersionId=178243829\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"88669b7f","metadata":{"papermill":{"duration":0.007793,"end_time":"2024-05-17T16:31:03.297197","exception":false,"start_time":"2024-05-17T16:31:03.289404","status":"completed"},"tags":[]},"source":["# Exercise 1: Basic Back Propagation"]},{"cell_type":"markdown","id":"35b79a6d","metadata":{"papermill":{"duration":0.006833,"end_time":"2024-05-17T16:31:03.311353","exception":false,"start_time":"2024-05-17T16:31:03.30452","status":"completed"},"tags":[]},"source":["### Step 1: Import Libraries\n","Import Numpy."]},{"cell_type":"code","execution_count":1,"id":"257a0b29","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:03.327965Z","iopub.status.busy":"2024-05-17T16:31:03.327132Z","iopub.status.idle":"2024-05-17T16:31:03.337877Z","shell.execute_reply":"2024-05-17T16:31:03.336886Z"},"papermill":{"duration":0.02175,"end_time":"2024-05-17T16:31:03.340146","exception":false,"start_time":"2024-05-17T16:31:03.318396","status":"completed"},"tags":[]},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","id":"512f5025","metadata":{"papermill":{"duration":0.006855,"end_time":"2024-05-17T16:31:03.3542","exception":false,"start_time":"2024-05-17T16:31:03.347345","status":"completed"},"tags":[]},"source":["### Step 2: Define the Network Architecture\n","Define the architecture of your neural network. It should have the following components:\n","- `input_size`: 1 input neuron. Think of this as the input data.\n","- `hidden_size`: 1 hidden neuron. This is an intermediate processing unit.\n","- `output_size`: 1 output neuron. This represents the network's prediction."]},{"cell_type":"code","execution_count":2,"id":"d8b846a9","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:03.370589Z","iopub.status.busy":"2024-05-17T16:31:03.36993Z","iopub.status.idle":"2024-05-17T16:31:03.373995Z","shell.execute_reply":"2024-05-17T16:31:03.373141Z"},"papermill":{"duration":0.014637,"end_time":"2024-05-17T16:31:03.375971","exception":false,"start_time":"2024-05-17T16:31:03.361334","status":"completed"},"tags":[]},"outputs":[],"source":["# Define the network architecture\n","input_size = 1\n","hidden_size = 1\n","output_size = 1"]},{"cell_type":"markdown","id":"9bb78361","metadata":{"papermill":{"duration":0.006898,"end_time":"2024-05-17T16:31:03.390099","exception":false,"start_time":"2024-05-17T16:31:03.383201","status":"completed"},"tags":[]},"source":["### Step 3: Initialize Weights and Biases\n","Initialize the parameters of the neural network with random values. The network learns by adjusting these parameters during training. Think of `W1` and `W2` as connection weights and `b1` and `b2` as biases."]},{"cell_type":"code","execution_count":3,"id":"7a3b7792","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:03.407263Z","iopub.status.busy":"2024-05-17T16:31:03.406605Z","iopub.status.idle":"2024-05-17T16:31:03.411513Z","shell.execute_reply":"2024-05-17T16:31:03.410769Z"},"papermill":{"duration":0.016371,"end_time":"2024-05-17T16:31:03.413485","exception":false,"start_time":"2024-05-17T16:31:03.397114","status":"completed"},"tags":[]},"outputs":[],"source":["np.random.seed(42)  # For reproducibility\n","\n","W1 = np.random.randn(input_size, hidden_size)\n","b1 = np.random.randn(hidden_size)\n","W2 = np.random.randn(hidden_size, output_size)\n","b2 = np.random.randn(output_size)\n"]},{"cell_type":"markdown","id":"a8f186b3","metadata":{"papermill":{"duration":0.007831,"end_time":"2024-05-17T16:31:03.429114","exception":false,"start_time":"2024-05-17T16:31:03.421283","status":"completed"},"tags":[]},"source":["### Step 4: Define the Activation Function (Sigmoid)\n","Define the sigmoid activation function. "]},{"cell_type":"code","execution_count":4,"id":"86ad084b","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:03.445397Z","iopub.status.busy":"2024-05-17T16:31:03.444725Z","iopub.status.idle":"2024-05-17T16:31:03.449246Z","shell.execute_reply":"2024-05-17T16:31:03.448284Z"},"papermill":{"duration":0.015298,"end_time":"2024-05-17T16:31:03.451561","exception":false,"start_time":"2024-05-17T16:31:03.436263","status":"completed"},"tags":[]},"outputs":[],"source":["def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))"]},{"cell_type":"markdown","id":"6eb5be27","metadata":{"papermill":{"duration":0.006895,"end_time":"2024-05-17T16:31:03.466019","exception":false,"start_time":"2024-05-17T16:31:03.459124","status":"completed"},"tags":[]},"source":["### Step 5: Forward Pass\n","The `forward` function computes the forward pass of the neural network. It takes an `input_data` and passes it through the network to generate an `output`. Here's what happens:\n","- `z1` is the result of multiplying the input data by the weight matrix `W1`, and then adding the bias `b1`. This is the input to the first (and only) hidden neuron.\n","- `a1` is the result of applying the sigmoid activation function to `z1`. This is the output of the hidden layer.\n","- `z2` is computed similarly using `a1`, `W2`, and `b2`. It's the input to the output neuron.\n","- Finally, `output` is the result of applying the sigmoid activation function to `z2`. This is the final output of the network."]},{"cell_type":"code","execution_count":5,"id":"e51cf87e","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:03.482342Z","iopub.status.busy":"2024-05-17T16:31:03.48196Z","iopub.status.idle":"2024-05-17T16:31:03.487019Z","shell.execute_reply":"2024-05-17T16:31:03.485929Z"},"papermill":{"duration":0.015946,"end_time":"2024-05-17T16:31:03.489144","exception":false,"start_time":"2024-05-17T16:31:03.473198","status":"completed"},"tags":[]},"outputs":[],"source":["def forward(input_data):\n","    z1 = np.dot(input_data, W1) + b1\n","    a1 = sigmoid(z1)\n","    z2 = np.dot(a1, W2) + b2\n","    output = sigmoid(z2)\n","    return output, a1"]},{"cell_type":"markdown","id":"2015214b","metadata":{"papermill":{"duration":0.006981,"end_time":"2024-05-17T16:31:03.503468","exception":false,"start_time":"2024-05-17T16:31:03.496487","status":"completed"},"tags":[]},"source":["### Step 6: Loss Function (Mean Squared Error)\n","The `loss` function computes the mean squared error between the predicted output and the target output. This tells us how far off our predictions are from the desired targets. During training, the goal is to minimize this loss."]},{"cell_type":"code","execution_count":6,"id":"07a2eca1","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:03.521361Z","iopub.status.busy":"2024-05-17T16:31:03.520561Z","iopub.status.idle":"2024-05-17T16:31:03.525708Z","shell.execute_reply":"2024-05-17T16:31:03.52498Z"},"papermill":{"duration":0.016175,"end_time":"2024-05-17T16:31:03.527905","exception":false,"start_time":"2024-05-17T16:31:03.51173","status":"completed"},"tags":[]},"outputs":[],"source":["# Loss Function (Mean Squared Error)\n","def loss(predicted_output, target_output):\n","    return np.mean((predicted_output - target_output) ** 2)"]},{"cell_type":"markdown","id":"9767e63f","metadata":{"papermill":{"duration":0.007055,"end_time":"2024-05-17T16:31:03.542228","exception":false,"start_time":"2024-05-17T16:31:03.535173","status":"completed"},"tags":[]},"source":["### Step 7: Backward Pass (Backpropagation)\n","The `backward` function implements backpropagation, which is the key to training the neural network. Here's a breakdown:\n","- `output` is the result of a forward pass through the network. It's used to compute the output error, which is the difference between the predicted output and the target output.\n","- `dW2` and `db2` are gradients that indicate how much the weights and biases in the output layer need to be adjusted to reduce the error.\n","- `hidden_error` is a measure of how much the hidden layer contributed to the error. It's used to compute `dW1` and `db1`, which represent the gradients for the weights and biases in the hidden layer.\n","- Finally, the weights and biases are updated using these gradients. The learning rate (`learning_rate`) controls the size of the updates."]},{"cell_type":"code","execution_count":7,"id":"31caf60f","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:03.558903Z","iopub.status.busy":"2024-05-17T16:31:03.558152Z","iopub.status.idle":"2024-05-17T16:31:03.565586Z","shell.execute_reply":"2024-05-17T16:31:03.564596Z"},"papermill":{"duration":0.01866,"end_time":"2024-05-17T16:31:03.568122","exception":false,"start_time":"2024-05-17T16:31:03.549462","status":"completed"},"tags":[]},"outputs":[],"source":["def backward(input_data, target_output, output, a1, learning_rate=0.1):\n","    global W1, b1, W2, b2  # Declare globals at the start\n","    \n","    output_error = output - target_output\n","    dW2 = np.dot(a1.T, output_error * output * (1 - output))\n","    db2 = np.sum(output_error * output * (1 - output), axis=0)\n","    \n","    hidden_error = np.dot(output_error * output * (1 - output), W2.T)\n","    dW1 = np.dot(input_data.T, hidden_error * a1 * (1 - a1))\n","    db1 = np.sum(hidden_error * a1 * (1 - a1), axis=0)\n","    \n","    W1 -= learning_rate * dW1\n","    b1 -= learning_rate * db1\n","    W2 -= learning_rate * dW2\n","    b2 -= learning_rate * db2\n"]},{"cell_type":"markdown","id":"0ba84930","metadata":{"papermill":{"duration":0.007381,"end_time":"2024-05-17T16:31:03.583321","exception":false,"start_time":"2024-05-17T16:31:03.57594","status":"completed"},"tags":[]},"source":["### Step 8: Training Loop\n","This is the training loop. It runs for a specified number of `num_epochs` and trains the neural network to make better predictions. The loop consists of the following steps:\n","- Input data (`input_data`) and target values (`target`) are defined. In this case, the input data is ` [0]` and `[1]` and the corresponding targets are also `[0]` and `[1]`.\n","- A forward pass is performed to get the predicted output.\n","- The current loss is computed using the `loss` function.\n","- Backpropagation is executed to adjust the weights and biases.\n","- Every 100 epochs, the current loss is printed to monitor the training progress."]},{"cell_type":"code","execution_count":8,"id":"9060436e","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:03.601003Z","iopub.status.busy":"2024-05-17T16:31:03.600276Z","iopub.status.idle":"2024-05-17T16:31:03.691799Z","shell.execute_reply":"2024-05-17T16:31:03.690914Z"},"papermill":{"duration":0.102936,"end_time":"2024-05-17T16:31:03.69419","exception":false,"start_time":"2024-05-17T16:31:03.591254","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 100, Loss: 0.2772080817526419\n","Epoch 200, Loss: 0.24919477102920234\n","Epoch 300, Loss: 0.24460396348390362\n","Epoch 400, Loss: 0.2398881473343858\n","Epoch 500, Loss: 0.23289412423845401\n","Epoch 600, Loss: 0.2226766568799951\n","Epoch 700, Loss: 0.2085477071770783\n","Epoch 800, Loss: 0.19029012839390697\n","Epoch 900, Loss: 0.16847095788115848\n","Epoch 1000, Loss: 0.14466209300946115\n"]}],"source":["input_data = np.array([[0], [1]])\n","target = np.array([[0], [1]])\n","\n","num_epochs = 1000\n","\n","for epoch in range(num_epochs):\n","    output, a1 = forward(input_data)\n","    current_loss = loss(output, target)\n","    backward(input_data, target, output, a1)\n","    \n","    if (epoch + 1) % 100 == 0:\n","        print(f'Epoch {epoch + 1}, Loss: {current_loss}')\n"]},{"cell_type":"markdown","id":"17259739","metadata":{"papermill":{"duration":0.007365,"end_time":"2024-05-17T16:31:03.709194","exception":false,"start_time":"2024-05-17T16:31:03.701829","status":"completed"},"tags":[]},"source":["### Step 9: Test the Trained Model\n","Once the training is done, we test the trained neural network with new data (`test_data`). We input 0.5 and see what the network predicts. This demonstrates that our neural network has learned to approximate certain patterns in the data."]},{"cell_type":"code","execution_count":9,"id":"0335a8e9","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:03.72635Z","iopub.status.busy":"2024-05-17T16:31:03.725648Z","iopub.status.idle":"2024-05-17T16:31:03.731833Z","shell.execute_reply":"2024-05-17T16:31:03.730549Z"},"papermill":{"duration":0.017202,"end_time":"2024-05-17T16:31:03.733913","exception":false,"start_time":"2024-05-17T16:31:03.716711","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Prediction for input 0.5: [[0.53594128]]\n"]}],"source":["test_data = np.array([[0.5]])\n","predicted_output, _ = forward(test_data)\n","print(f'Prediction for input 0.5: {predicted_output}')\n"]},{"cell_type":"markdown","id":"bc54d19c","metadata":{"papermill":{"duration":0.00743,"end_time":"2024-05-17T16:31:03.749071","exception":false,"start_time":"2024-05-17T16:31:03.741641","status":"completed"},"tags":[]},"source":["# Exercise 2: Back Propagation for Multi-layer Neural Network"]},{"cell_type":"markdown","id":"63ca5edb","metadata":{"papermill":{"duration":0.007753,"end_time":"2024-05-17T16:31:03.764434","exception":false,"start_time":"2024-05-17T16:31:03.756681","status":"completed"},"tags":[]},"source":["### Step 1. Importing Libraries:\n","Import any necessary libraries."]},{"cell_type":"code","execution_count":10,"id":"b18b5501","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:03.781608Z","iopub.status.busy":"2024-05-17T16:31:03.780899Z","iopub.status.idle":"2024-05-17T16:31:05.116391Z","shell.execute_reply":"2024-05-17T16:31:05.11541Z"},"papermill":{"duration":1.346765,"end_time":"2024-05-17T16:31:05.118751","exception":false,"start_time":"2024-05-17T16:31:03.771986","status":"completed"},"tags":[]},"outputs":[],"source":["import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n"]},{"cell_type":"markdown","id":"8b35f321","metadata":{"papermill":{"duration":0.007415,"end_time":"2024-05-17T16:31:05.134246","exception":false,"start_time":"2024-05-17T16:31:05.126831","status":"completed"},"tags":[]},"source":["### Step 2. Defining Network Architecture:\n","Here, you define the architecture of the neural network:\n","    • `input_size`: The number of input neurons, set to 4.\n","    • `hidden_size`: The number of neurons in the hidden layer, it’s a hyperparameter.\n","    • `output_size`: The number of output neurons, set to 3."]},{"cell_type":"code","execution_count":11,"id":"2123cf4e","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:05.152015Z","iopub.status.busy":"2024-05-17T16:31:05.151133Z","iopub.status.idle":"2024-05-17T16:31:05.156216Z","shell.execute_reply":"2024-05-17T16:31:05.155149Z"},"papermill":{"duration":0.016299,"end_time":"2024-05-17T16:31:05.158311","exception":false,"start_time":"2024-05-17T16:31:05.142012","status":"completed"},"tags":[]},"outputs":[],"source":["input_size = 4       # Number of input neurons (features in IRIS dataset)\n","hidden_size = 5      # Number of neurons in the hidden layer (hyperparameter, can be adjusted)\n","output_size = 3      # Number of output neurons (classes in IRIS dataset)\n"]},{"cell_type":"markdown","id":"25093a29","metadata":{"papermill":{"duration":0.007519,"end_time":"2024-05-17T16:31:05.173674","exception":false,"start_time":"2024-05-17T16:31:05.166155","status":"completed"},"tags":[]},"source":["### Step 3. Initializing Weights and Biases:\n","Here, you initialize the weights and biases with random values. These parameters will be adjusted during training to make the network learn."]},{"cell_type":"code","execution_count":12,"id":"a84ee4a0","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:05.191483Z","iopub.status.busy":"2024-05-17T16:31:05.190867Z","iopub.status.idle":"2024-05-17T16:31:05.196421Z","shell.execute_reply":"2024-05-17T16:31:05.195615Z"},"papermill":{"duration":0.016809,"end_time":"2024-05-17T16:31:05.198529","exception":false,"start_time":"2024-05-17T16:31:05.18172","status":"completed"},"tags":[]},"outputs":[],"source":["np.random.seed(42)  # For reproducibility\n","\n","W1 = np.random.randn(input_size, hidden_size)\n","b1 = np.random.randn(hidden_size)\n","W2 = np.random.randn(hidden_size, output_size)\n","b2 = np.random.randn(output_size)\n"]},{"cell_type":"markdown","id":"2a40601b","metadata":{"papermill":{"duration":0.007754,"end_time":"2024-05-17T16:31:05.214079","exception":false,"start_time":"2024-05-17T16:31:05.206325","status":"completed"},"tags":[]},"source":["### Step 4. Defining the Activation Function:\n","Define appropriate activation functions for each layer."]},{"cell_type":"code","execution_count":13,"id":"736c8d7f","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:05.231761Z","iopub.status.busy":"2024-05-17T16:31:05.231137Z","iopub.status.idle":"2024-05-17T16:31:05.236534Z","shell.execute_reply":"2024-05-17T16:31:05.235797Z"},"papermill":{"duration":0.016752,"end_time":"2024-05-17T16:31:05.238628","exception":false,"start_time":"2024-05-17T16:31:05.221876","status":"completed"},"tags":[]},"outputs":[],"source":["def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def softmax(x):\n","    exp_x = np.exp(x - np.max(x))\n","    return exp_x / exp_x.sum(axis=1, keepdims=True)\n"]},{"cell_type":"markdown","id":"ba4f3794","metadata":{"papermill":{"duration":0.007602,"end_time":"2024-05-17T16:31:05.254153","exception":false,"start_time":"2024-05-17T16:31:05.246551","status":"completed"},"tags":[]},"source":["### Step 5. Forward Pass:\n","This function performs the forward pass through the neural network."]},{"cell_type":"code","execution_count":14,"id":"69e75d0a","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:05.272394Z","iopub.status.busy":"2024-05-17T16:31:05.271217Z","iopub.status.idle":"2024-05-17T16:31:05.277234Z","shell.execute_reply":"2024-05-17T16:31:05.27612Z"},"papermill":{"duration":0.017641,"end_time":"2024-05-17T16:31:05.279688","exception":false,"start_time":"2024-05-17T16:31:05.262047","status":"completed"},"tags":[]},"outputs":[],"source":["def forward(input_data):\n","    z1 = np.dot(input_data, W1) + b1\n","    a1 = sigmoid(z1)\n","    z2 = np.dot(a1, W2) + b2\n","    output = softmax(z2)\n","    return output, a1\n"]},{"cell_type":"markdown","id":"311972a2","metadata":{"papermill":{"duration":0.007686,"end_time":"2024-05-17T16:31:05.295376","exception":false,"start_time":"2024-05-17T16:31:05.28769","status":"completed"},"tags":[]},"source":["### Step 6. Loss Function (Cross Entropy Loss):\n","The `loss` function calculates the cross-entropy loss between the predicted output and the target output. This measures how well the network is performing, and during training, the goal is to minimize this error."]},{"cell_type":"code","execution_count":15,"id":"ba29f880","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:05.312799Z","iopub.status.busy":"2024-05-17T16:31:05.312414Z","iopub.status.idle":"2024-05-17T16:31:05.318073Z","shell.execute_reply":"2024-05-17T16:31:05.317026Z"},"papermill":{"duration":0.016688,"end_time":"2024-05-17T16:31:05.3201","exception":false,"start_time":"2024-05-17T16:31:05.303412","status":"completed"},"tags":[]},"outputs":[],"source":["def loss(predicted_output, target_output):\n","    m = target_output.shape[0]\n","    log_likelihood = -np.log(predicted_output[range(m), target_output.argmax(axis=1)])\n","    return np.sum(log_likelihood) / m\n"]},{"cell_type":"markdown","id":"3a5e7aa3","metadata":{"papermill":{"duration":0.007833,"end_time":"2024-05-17T16:31:05.335927","exception":false,"start_time":"2024-05-17T16:31:05.328094","status":"completed"},"tags":[]},"source":["### Step 7. Backward Pass (Backpropagation):\n","The `backward` function implements backpropagation to update the weights and biases of the neural network. "]},{"cell_type":"code","execution_count":16,"id":"7f5d36ad","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:05.353956Z","iopub.status.busy":"2024-05-17T16:31:05.353257Z","iopub.status.idle":"2024-05-17T16:31:05.362057Z","shell.execute_reply":"2024-05-17T16:31:05.36089Z"},"papermill":{"duration":0.020658,"end_time":"2024-05-17T16:31:05.364419","exception":false,"start_time":"2024-05-17T16:31:05.343761","status":"completed"},"tags":[]},"outputs":[],"source":["def backward(input_data, target_output, output, a1, learning_rate=0.1):\n","    global W1, b1, W2, b2  # Declare globals at the start\n","    \n","    m = input_data.shape[0]\n","\n","    output_error = output - target_output\n","    dW2 = np.dot(a1.T, output_error) / m\n","    db2 = np.sum(output_error, axis=0) / m\n","    \n","    hidden_error = np.dot(output_error, W2.T) * a1 * (1 - a1)\n","    dW1 = np.dot(input_data.T, hidden_error) / m\n","    db1 = np.sum(hidden_error, axis=0) / m\n","    \n","    W1 -= learning_rate * dW1\n","    b1 -= learning_rate * db1\n","    W2 -= learning_rate * dW2\n","    b2 -= learning_rate * db2\n"]},{"cell_type":"markdown","id":"9a206ec9","metadata":{"papermill":{"duration":0.007757,"end_time":"2024-05-17T16:31:05.380367","exception":false,"start_time":"2024-05-17T16:31:05.37261","status":"completed"},"tags":[]},"source":["### Step 8. Training Loop:\n","This section runs a training loop for a specified number of epochs (`num_epochs`). In each epoch:\n","- Input data (`input_data`) and target values (`target`) are defined. \n","- A forward pass is performed to get the predicted output and the intermediate activations.\n","- The current loss is computed using the `loss` function.\n","- Backpropagation is executed to adjust the weights and biases based on the error."]},{"cell_type":"code","execution_count":17,"id":"bc812cac","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:05.398011Z","iopub.status.busy":"2024-05-17T16:31:05.397547Z","iopub.status.idle":"2024-05-17T16:31:05.557522Z","shell.execute_reply":"2024-05-17T16:31:05.556216Z"},"papermill":{"duration":0.172333,"end_time":"2024-05-17T16:31:05.560594","exception":false,"start_time":"2024-05-17T16:31:05.388261","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 100, Loss: 0.7258009559928861\n","Epoch 200, Loss: 0.6099738404180975\n","Epoch 300, Loss: 0.5557907540900652\n","Epoch 400, Loss: 0.5269849886784278\n","Epoch 500, Loss: 0.5100887697979113\n","Epoch 600, Loss: 0.49916775267736113\n","Epoch 700, Loss: 0.49136864163446153\n","Epoch 800, Loss: 0.4850218876415384\n","Epoch 900, Loss: 0.4783990161707043\n","Epoch 1000, Loss: 0.4649933768182768\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]}],"source":["# Load and preprocess the IRIS dataset\n","iris = load_iris()\n","input_data = iris.data\n","target = iris.target.reshape(-1, 1)\n","\n","# One hot encode the target values\n","encoder = OneHotEncoder(sparse=False)\n","target = encoder.fit_transform(target)\n","\n","# Split the data into training and testing sets\n","train_data, test_data, train_target, test_target = train_test_split(input_data, target, test_size=0.2, random_state=42)\n","\n","num_epochs = 1000\n","\n","for epoch in range(num_epochs):\n","    output, a1 = forward(train_data)\n","    current_loss = loss(output, train_target)\n","    backward(train_data, train_target, output, a1)\n","    \n","    if (epoch + 1) % 100 == 0:\n","        print(f'Epoch {epoch + 1}, Loss: {current_loss}')\n"]},{"cell_type":"markdown","id":"2691a339","metadata":{"papermill":{"duration":0.007926,"end_time":"2024-05-17T16:31:05.576652","exception":false,"start_time":"2024-05-17T16:31:05.568726","status":"completed"},"tags":[]},"source":["### Step 9. Testing the Trained Model:\n","After training, the code tests the trained model on the validation set using the `test_data`. It prints the performance metrics."]},{"cell_type":"code","execution_count":18,"id":"792717c8","metadata":{"execution":{"iopub.execute_input":"2024-05-17T16:31:05.595327Z","iopub.status.busy":"2024-05-17T16:31:05.594956Z","iopub.status.idle":"2024-05-17T16:31:05.601691Z","shell.execute_reply":"2024-05-17T16:31:05.600607Z"},"papermill":{"duration":0.018967,"end_time":"2024-05-17T16:31:05.60398","exception":false,"start_time":"2024-05-17T16:31:05.585013","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 100.00%\n"]}],"source":["def predict(input_data):\n","    output, _ = forward(input_data)\n","    return np.argmax(output, axis=1)\n","\n","# Predict on test data\n","predictions = predict(test_data)\n","accuracy = np.mean(predictions == test_target.argmax(axis=1))\n","print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"]},{"cell_type":"code","execution_count":null,"id":"eb1b99e8","metadata":{"papermill":{"duration":0.008052,"end_time":"2024-05-17T16:31:05.620334","exception":false,"start_time":"2024-05-17T16:31:05.612282","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"954c945a","metadata":{"papermill":{"duration":0.008142,"end_time":"2024-05-17T16:31:05.636716","exception":false,"start_time":"2024-05-17T16:31:05.628574","status":"completed"},"tags":[]},"source":["**Find More Labs**\n","\n","This lab is from my Machine Learning Course, that is a part of my [Software Engineering](https://seecs.nust.edu.pk/program/bachelor-of-software-engineering-for-fall-2021-onward) Degree at [NUST](https://nust.edu.pk).\n","\n","The content in the provided list of notebooks covers a range of topics in **machine learning** and **data analysis** implemented from scratch or using popular libraries like **NumPy**, **pandas**, **scikit-learn**, **seaborn**, and **matplotlib**. It includes introductory materials on NumPy showcasing its efficiency for mathematical operations, **linear regression**, **logistic regression**, **decision trees**, **K-nearest neighbors (KNN)**, **support vector machines (SVM)**, **Naive Bayes**, **K-means** clustering, principle component analysis (**PCA**), and **neural networks** with **backpropagation**. Each notebook demonstrates practical implementation and application of these algorithms on various datasets such as the **California Housing** Dataset, **MNIST** dataset, **Iris** dataset, **Auto-MPG** dataset, and the **UCI Adult Census Income** dataset. Additionally, it covers topics like **gradient descent optimization**, model evaluation metrics (e.g., **accuracy, precision, recall, f1 score**), **regularization** techniques (e.g., **Lasso**, **Ridge**), and **data visualization**.\n","\n","| Title                                                                                                                   | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n","| ----------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n","| [01 - Intro to Numpy](https://www.kaggle.com/code/sacrum/ml-labs-01-intro-to-numpy)                                     | The notebook demonstrates NumPy's efficiency for mathematical operations like array `reshaping`, `sigmoid`, `softmax`, `dot` and `outer products`, `L1 and L2 losses`, and matrix operations. It highlights NumPy's superiority over standard Python lists in speed and convenience for scientific computing and machine learning tasks.                                                                                                                                                                                              |\n","| [02 - Linear Regression From Scratch](https://www.kaggle.com/code/sacrum/ml-labs-02-linear-regression-from-scratch)     | This notebook implements `linear regression` and `gradient descent` from scratch in Python using `NumPy`, focusing on predicting house prices with the `California Housing Dataset`. It defines functions for prediction, `MSE` calculation, and gradient computation. Batch gradient descent is used for optimization. The dataset is loaded, scaled, and split. `Batch, stochastic, and mini-batch gradient descents` are applied with varying hyperparameters. Finally, the MSEs of the predictions from each method are compared. |\n","| [03 - Logistic Regression from Scratch](https://www.kaggle.com/code/sacrum/ml-labs-03-logistic-regression-from-scratch) | This notebook outlines the implementation of `logistic regression` from scratch in Python using `NumPy`, including functions for prediction, loss calculation, gradient computation, and batch `gradient descent` optimization, applied to the `MNIST` dataset for handwritten digit recognition and `Iris` data. And also inclues metrics like `accuracy`, `precision`, `recall`, `f1 score`                                                                                                                                         |\n","| [04 - Auto-MPG Regression](https://www.kaggle.com/code/sacrum/ml-labs-04-auto-mpg-regression)                           | The notebook uses `pandas` for data manipulation, `seaborn` and `matplotlib` for visualization, and `sklearn` for `linear regression` and `regularization` techniques (`Lasso` and `Ridge`). It includes data loading, processing, visualization, model training, and evaluation on the `Auto-MPG dataset`.                                                                                                                                                                                                                           |\n","| [05 - Desicion Trees from Scratch](https://www.kaggle.com/code/sacrum/ml-labs-05-desicion-trees-from-scratch)           | In this notebook, `DecisionTree` algorithm has been implmented from scratch and applied on dummy dataset                                                                                                                                                                                                                                                                                                                                                                                                                              |\n","| [06 - KNN from Scratch](https://www.kaggle.com/code/sacrum/ml-labs-06-knn-from-scratch)                                 | In this notebook, `K-Nearest Neighbour` algorithm has been implemented from scratch and compared with KNN provided in scikit-learn package                                                                                                                                                                                                                                                                                                                                                                                            |\n","| [07 - SVM](https://www.kaggle.com/code/sacrum/ml-labs-07-svm)                                                           | This notebook implements `SVM classifier` on `Iris Dataset`                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n","| [08 - Naive Bayes](https://www.kaggle.com/code/sacrum/ml-labs-08-naive-bayes)                                           | This notebook trains `Naive Bayes` and compares it with other algorithms `Decision Trees`, `SVM` and `Logistic Regression`                                                                                                                                                                                                                                                                                                                                                                                                            |\n","| [09 - K-means](https://www.kaggle.com/code/sacrum/ml-labs-09-k-means)                                                   | In this notebook `K-means` algorithm has been implemented using `scikit-learn` and different values of `k` are compared to understand the `elbow method` in `Calinski Harabasz Scores`                                                                                                                                                                                                                                                                                                                                                |\n","| [10 - UCI Adult Census Income](https://www.kaggle.com/code/sacrum/ml-labs-10-uci-adult-census-income)                   | Here I have used the UCI Adult Income dataset and applied different machine learning algorithms to find the best model configuration for predicting salary from the given information                                                                                                                                                                                                                                                                                                                                                 |\n","| [11 - PCA](https://www.kaggle.com/code/sacrum/ml-labs-11-pca)                                                           | `Principle Component Analysis` implemented from scratch                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n","| [12 - Neural Networks](https://www.kaggle.com/code/sacrum/ml-labs-12-neural-networks)                                   | This code implements neural networks with back propagation from scratch                                                                                                                                                                                                                                                                                                                                                                                                                                                               |"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":19,"sourceId":420,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"ai","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":5.606062,"end_time":"2024-05-17T16:31:06.166385","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-17T16:31:00.560323","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}